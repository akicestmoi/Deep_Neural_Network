{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bita6faa0f504cb47b9b202f4621b79fec8",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# __Convolutional Neural Network from scratch__\n",
    "### _Author: Aki Taniguchi_\n",
    "### _Original date: 19/02/2020_\n",
    "### _Last update: 19/02/2020_\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Setting environment\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:\\\\Users\\\\tngch\\\\Python Code and AI\\\\Neural Network\")\n",
    "\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from Deep_Neural_Network import activation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(3, 32, 32, 10)\n",
    "Y = np.random.choice(2, 10)\n",
    "\n",
    "A = {}\n",
    "A['0'] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the hyper-parameters (\"LeNet-5\")\n",
    "architecture = ['init', 'conv', 'maxpool', 'conv', 'avgpool', 'fc', 'fc']\n",
    "activations = ['init', \"relu\", 'none', 'relu', 'none', 'sigmoid']\n",
    "filter_size = [0, 5, 2, 5, 2, 1, 1]\n",
    "nb_kernel = [3, 8, 8, 16, 16, 120, 84] # first is RGB, fully connected layers are the number of neurons\n",
    "padding = [0, 0, 0, 0, 0, 0, 0]\n",
    "stride = [0, 1, 2, 1, 2, 0, 0]\n",
    "L = len(architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Initialize models\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Shape of Output A. Note that the layer with full connection needs to be vectorized, but won't be here\n",
    "# dim A is (channel, height, width, observation)\n",
    "# width and height are both determined: int[(x + 2p - f) / s + 1]\n",
    "def get_layer_shape(architecture, filter_size, nb_kernel, padding, stride, A):\n",
    "\n",
    "    layer_shape = [A['0'].shape]\n",
    "\n",
    "    m = A['0'].shape[3]\n",
    "    n_h = A['0'].shape[1]\n",
    "    n_w = A['0'].shape[2]\n",
    "    \n",
    "    L = len(architecture)\n",
    "    f = filter_size\n",
    "    k = nb_kernel\n",
    "    p = padding\n",
    "    s = stride\n",
    "\n",
    "    # Note the last layer has only 2 outcome as we are not doing a softmax. This needs to be changed once it will be implemented.\n",
    "    for l in range(1, L+1):\n",
    "        if l != L:\n",
    "            if architecture[l] != 'fc':\n",
    "                n_h = int((n_h + 2*p[l] - f[l]) / s[l] + 1)\n",
    "                n_w = int((n_w + 2*p[l] - f[l]) / s[l] + 1)\n",
    "                layer_shape.append((k[l], n_h, n_w, m))\n",
    "            else:\n",
    "                layer_shape.append((k[l], m))\n",
    "        else:\n",
    "            layer_shape.append((2, m))\n",
    "\n",
    "    return layer_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(3, 32, 32, 10),\n (8, 28, 28, 10),\n (8, 14, 14, 10),\n (16, 10, 10, 10),\n (16, 5, 5, 10),\n (120, 10),\n (84, 10),\n (2, 10)]"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test get_layer_shape\n",
    "layer_shape = get_layer_shape(architecture, filter_size, nb_kernel, padding, stride, A)\n",
    "\n",
    "layer_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters size:\n",
    "# If Conv/Pool: (curr channel, prev channel, filter, filter) and (cur channel, 1, 1, 1)\n",
    "# If FC from Conv/Pool: (curr channel, vectorized[prev output])\n",
    "# If FC from FC: (curr channel, prev channel)\n",
    "# bias is always (curr channel, 1) when FC\n",
    "def initialize_model(architecture, filter_size, nb_kernel, padding, stride, A):\n",
    "\n",
    "    W = {}; b = {}\n",
    "    L = len(architecture)\n",
    "    f = filter_size\n",
    "    k = nb_kernel\n",
    "    m = A['0'].shape[3]\n",
    "    layer_shape = get_layer_shape(architecture, filter_size, nb_kernel, padding, stride, A)\n",
    "\n",
    "    for l in range(1, L):\n",
    "\n",
    "        # We need to initialize output as well to allocate the convolution output per index (otherwise Python doesn't allow allocation)\n",
    "        A[str(l)] = np.zeros((layer_shape[l]))\n",
    "\n",
    "        # Now initializing parameters\n",
    "        if architecture[l] != 'fc':\n",
    "            W[str(l)] = np.random.randn(k[l], k[l-1], f[l], f[l]) * 0.01\n",
    "            b[str(l)] = np.zeros((k[l], 1, 1, 1))\n",
    "\n",
    "        else:\n",
    "            # Parameters size different at the moment of change from conv to fc due to vectorized output\n",
    "            if architecture[l-1] != 'fc':\n",
    "                W[str(l)] = np.random.randn(k[l], int(np.prod(A[str(l-1)].shape) / m))\n",
    "            else:\n",
    "                W[str(l)] = np.random.randn(k[l], k[l-1])\n",
    "            \n",
    "            b[str(l)] = np.zeros((k[l], 1))\n",
    "\n",
    "    return A, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "init\nA[0] shape: (3, 32, 32, 10)\nconv\nA[1] shape: (8, 28, 28, 10)\nW[1] shape: (8, 3, 5, 5)\nb[1] shape: (8, 1, 1, 1)\nmaxpool\nA[2] shape: (8, 14, 14, 10)\nW[2] shape: (8, 8, 2, 2)\nb[2] shape: (8, 1, 1, 1)\nconv\nA[3] shape: (16, 10, 10, 10)\nW[3] shape: (16, 8, 5, 5)\nb[3] shape: (16, 1, 1, 1)\navgpool\nA[4] shape: (16, 5, 5, 10)\nW[4] shape: (16, 16, 2, 2)\nb[4] shape: (16, 1, 1, 1)\nfc\nA[5] shape: (120, 10)\nW[5] shape: (120, 400)\nb[5] shape: (120, 1)\nfc\nA[6] shape: (84, 10)\nW[6] shape: (84, 120)\nb[6] shape: (84, 1)\n"
    }
   ],
   "source": [
    "# Test initialize_model\n",
    "A, W, b = initialize_model(architecture, filter_size, nb_kernel, padding, stride, A)\n",
    "\n",
    "for l in range(L):\n",
    "    print(architecture[l])\n",
    "    print(\"A[{0}] shape: {1}\".format(l, A[str(l)].shape))\n",
    "    if l != 0:\n",
    "        print(\"W[{0}] shape: {1}\".format(l, W[str(l)].shape))\n",
    "        print(\"b[{0}] shape: {1}\".format(l, b[str(l)].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Forward prop\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We won't be using numpy's pad function as this one is enough and quick to operate\n",
    "# Only works for 3D matrix\n",
    "def add_padding(A, padding, value=0, axis=(0, 1, 2)):\n",
    "\n",
    "    kernel, height, width = axis\n",
    "\n",
    "    horizontal_pad = np.zeros((A.shape[kernel], padding, A.shape[width] + 2*padding)) + value\n",
    "    vertical_pad = np.zeros((A.shape[kernel], A.shape[height], padding)) + value\n",
    "\n",
    "    padded_A = np.concatenate((vertical_pad, A), axis=width)\n",
    "    padded_A = np.concatenate((padded_A, vertical_pad), axis=width)\n",
    "    padded_A = np.concatenate((horizontal_pad, padded_A), axis=height)\n",
    "    padded_A = np.concatenate((padded_A, horizontal_pad), axis=height)\n",
    "\n",
    "    return padded_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Original matrix shape: (3, 32, 32)\nPadded matrix: (3, 34, 34)\n___________________\nOriginal matrix: [[[0.68211589 0.16431628 0.21873957 ... 0.4513942  0.79306114 0.32373973]\n  [0.96853521 0.15451759 0.25596092 ... 0.88739295 0.98164033 0.61304932]\n  [0.82797514 0.23876771 0.26718728 ... 0.8370273  0.10725634 0.24952633]\n  ...\n  [0.80530619 0.39678488 0.02501216 ... 0.05313387 0.65240194 0.18754455]\n  [0.44044975 0.9188455  0.44336157 ... 0.46367228 0.82713147 0.49512589]\n  [0.41804523 0.90447351 0.54907265 ... 0.86258199 0.6044871  0.99689135]]\n\n [[0.3581536  0.79625069 0.6710511  ... 0.53648894 0.39812714 0.74356902]\n  [0.3665738  0.3830787  0.4441578  ... 0.51962161 0.02578538 0.23988508]\n  [0.47629511 0.15162973 0.09900973 ... 0.88211338 0.03120495 0.98345109]\n  ...\n  [0.03603372 0.23484851 0.90752172 ... 0.19738284 0.50216651 0.09663376]\n  [0.86721578 0.10694605 0.0093212  ... 0.86104792 0.96772179 0.8318124 ]\n  [0.02447807 0.89184239 0.54213392 ... 0.65143337 0.37996596 0.53056101]]\n\n [[0.01638674 0.3887927  0.7755623  ... 0.63984874 0.2488141  0.77143083]\n  [0.76467279 0.48775387 0.85481024 ... 0.37844262 0.17849456 0.15052996]\n  [0.90334317 0.18224374 0.83043967 ... 0.19150552 0.63022723 0.47993544]\n  ...\n  [0.00609993 0.12208349 0.30740005 ... 0.67468697 0.04276546 0.90617157]\n  [0.02378399 0.68740333 0.63211629 ... 0.18209617 0.55504085 0.47602227]\n  [0.30197041 0.56467067 0.47493393 ... 0.42689948 0.39541085 0.94169093]]]\n___________________\nPadded matrix: [[[0.         0.         0.         ... 0.         0.         0.        ]\n  [0.         0.68211589 0.16431628 ... 0.79306114 0.32373973 0.        ]\n  [0.         0.96853521 0.15451759 ... 0.98164033 0.61304932 0.        ]\n  ...\n  [0.         0.44044975 0.9188455  ... 0.82713147 0.49512589 0.        ]\n  [0.         0.41804523 0.90447351 ... 0.6044871  0.99689135 0.        ]\n  [0.         0.         0.         ... 0.         0.         0.        ]]\n\n [[0.         0.         0.         ... 0.         0.         0.        ]\n  [0.         0.3581536  0.79625069 ... 0.39812714 0.74356902 0.        ]\n  [0.         0.3665738  0.3830787  ... 0.02578538 0.23988508 0.        ]\n  ...\n  [0.         0.86721578 0.10694605 ... 0.96772179 0.8318124  0.        ]\n  [0.         0.02447807 0.89184239 ... 0.37996596 0.53056101 0.        ]\n  [0.         0.         0.         ... 0.         0.         0.        ]]\n\n [[0.         0.         0.         ... 0.         0.         0.        ]\n  [0.         0.01638674 0.3887927  ... 0.2488141  0.77143083 0.        ]\n  [0.         0.76467279 0.48775387 ... 0.17849456 0.15052996 0.        ]\n  ...\n  [0.         0.02378399 0.68740333 ... 0.55504085 0.47602227 0.        ]\n  [0.         0.30197041 0.56467067 ... 0.39541085 0.94169093 0.        ]\n  [0.         0.         0.         ... 0.         0.         0.        ]]]\n"
    }
   ],
   "source": [
    "# Test add_padding\n",
    "padded_matrix = add_padding(A['0'][:,:,:,0], 1, value=0, axis=(0, 1, 2))\n",
    "print(\"Original matrix shape:\", A['0'][:,:,:,0].shape)\n",
    "print(\"Padded matrix:\", padded_matrix.shape)\n",
    "print(\"___________________\")\n",
    "print(\"Original matrix:\", A['0'][:,:,:,0])\n",
    "print(\"___________________\")\n",
    "print(\"Padded matrix:\", padded_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this only works for a 4D matrix\n",
    "def slicing(A, layer, observation, height, width, filter_size, padding, stride):\n",
    "\n",
    "    # Padding the matrix to work the slicing on\n",
    "    # This is base matrix where we are going to create all the necessary slicing (hence the need to pad it first)\n",
    "    # Also note that we don't need to specify the value of kernel, as we won't slice through the depth (instead we take it all)\n",
    "    padded_A = add_padding(A['0'][:, :, :, observation], padding[layer], value=0, axis=(0, 1, 2))\n",
    "    f = filter_size\n",
    "    s = stride\n",
    "\n",
    "    for i in range(height):\n",
    "        v1 = i * s[layer]\n",
    "        v2 = i * s[layer] + f[layer]\n",
    "\n",
    "        for j in range(width):\n",
    "            h1 = j * s[layer]\n",
    "            h2 = j * s[layer] + f[layer]\n",
    "\n",
    "            slice = padded_A[:, v1:v2, h1:h2, observation]\n",
    "\n",
    "            yield slice, i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test slicing\n",
    "\n",
    "for slice, i, j in slicing(A, layer, observation, height, width, filter_size, padding, stride):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_forward_propagation():\n",
    "    \n",
    "    for m in range(observation):\n",
    "\n",
    "        for l in range(layer):\n",
    "\n",
    "            for k in range(kernel):\n",
    "                if architecture[l] == 'conv':\n",
    "                    for slice, i, j in slicing():\n",
    "                        Z[k, i, j, m] = np.sum(np.prod(slice, W[str(l)])) + b[str(l)]\n",
    "                        A[k, i, j, m] = activation_function(activations, Z[k, i, j, m])\n",
    "                \n",
    "                if architecture[l] == 'maxpool':\n",
    "                    for slice, i, j in slicing():\n",
    "                        A[k, i, j, m] = np.max(slice)\n",
    "\n",
    "                if architecture[l] == 'avgpool'\n",
    "                    for slice, i, j in slicing():\n",
    "                        A[k, i, j, m] = np.mean(slice)\n",
    "\n",
    "    for l in range(layer):\n",
    "        if architecture[l] == 'fc':\n",
    "            # Vectorize and calculate forward prop\n",
    "\n",
    "\n",
    "    return A         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward prop\n",
    "convolutional_forward_propagation()"
   ]
  }
 ]
}