# Coding Deep Neural Network from scratch
This git repository is designed to store my Jupyter code which sums up all the theory (with Math and demonstration) and code of different Deep Neural Network Architecture. It shows how to build a Deep Neural Network from scratch, including all the Maths needed to understand what it does. It requires basic understanding of Linear Algrebra and Calculus.

Note that I constantly update the content, please do not hesitate to contact me if you find any typo or wrong explanation.
For educational purposes, neither Tensorflow nor Keras (or the like) is used so that the code reflects all steps demonstrated in the theory (only numpy is used for optimization sake).

## Table of Content:
### I. Deep Neural Network: 
  - Initialization of Parameters (To do: Xavier initialization)
  - Forward propagation
  - Backward propagation
  - Optimization (Momentum, RMSProp, ADAM)
  - Batching (Mini-batch, Stochastic)
  - Normalization (To do)
  - Regularization (To do: L2, Drop-out)

### II. Convolutional Neural Network
  - Initialization of Parameters and Output
  - Convolutional Forward propagation
  - Pooling
  - Backward propagation (To do)
  - Residual Network (To do)
  - Inception Network (To do)
  - You Only Look Once (To do)
  
### III. Recurrent Neural Network (To do)
  
