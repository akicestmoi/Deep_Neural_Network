# Coding Deep Neural Network from scratch
This git repository is designed to store my Jupyter code which sums up all theory (with Math and demonstration) and code of Deep Neural Network

It shows how to build a Deep Neural Network from scratch, including all the Maths needed to understand what it does.
It requires basic understanding of Linear Algrebra and Calculus.

Note that I constantly update the content, please do not hesitate to contact me if you find any typo or wrong explanation.
Note that neither Tensorflow nor Keras (or the like) is used so that we code the full structure from scratch.

## Table of Content:
### I. Deep Neural Network: 
  - Initialization of Parameters (To do: Xavier initialization)
  - Forward propagation
  - Backward propagation
  - Optimization (Momentum, RMSProp, ADAM)
  - Batching (Mini-batch, Stochastic)
  - Normalization (To do)
  - Regularization (To do: L2, Drop-out)

### II. Convolutional Neural Network
  - Initialization of Parameters and Output
  - Convolutional Forward propagation
  - Pooling
  - Backward propagation (To do)
  - Residual Network (To do)
  - Inception Network (To do)
  - You Only Look Once (To do)
  
### III. Recurrent Neural Network (To do)
  
