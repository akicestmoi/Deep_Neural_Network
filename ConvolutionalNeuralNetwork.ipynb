{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bita6faa0f504cb47b9b202f4621b79fec8",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "___\n",
    "# __Convolutional Neural Network from scratch__\n",
    "### _Author: Aki Taniguchi_\n",
    "### _Original date: 19/02/2020_\n",
    "### _Last update: 25/02/2020_\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I. Setting environment\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:\\\\Users\\\\tngch\\\\Python Code and AI\\\\Neural Network\")\n",
    "\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from Deep_Neural_Network import activation_function\n",
    "from Deep_Neural_Network import compute_cost\n",
    "from Deep_Neural_Network import backward_activation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(3, 32, 32, 10)\n",
    "Y = np.random.choice(2, 10)\n",
    "\n",
    "A = {}\n",
    "A['0'] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the hyper-parameters (\"LeNet-5\")\n",
    "architecture = ['init', 'conv', 'maxpool', 'conv', 'avgpool', 'fc', 'fc']\n",
    "activations = ['init', 'relu', 'none', 'relu', 'none', 'relu', 'sigmoid']\n",
    "filter_size = [0, 5, 2, 5, 2, 1, 1]\n",
    "nb_kernel = [3, 8, 8, 16, 16, 120, 84] # first is RGB, fully connected layers are the number of neurons\n",
    "padding = [0, 0, 0, 0, 0, 0, 0]\n",
    "stride = [0, 1, 2, 1, 2, 0, 0]\n",
    "L = len(architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## II. Initialize model\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Shape of Output A. Note that the layer with full connection needs to be vectorized, but won't be here\n",
    "# dim A is (channel, height, width, observation)\n",
    "# width and height are both determined: int[(x + 2p - f) / s + 1]\n",
    "def get_layer_shape(architecture, filter_size, nb_kernel, padding, stride, A):\n",
    "\n",
    "    layer_shape = [A['0'].shape]\n",
    "\n",
    "    m = A['0'].shape[3]\n",
    "    n_h = A['0'].shape[1]\n",
    "    n_w = A['0'].shape[2]\n",
    "    \n",
    "    L = len(architecture)\n",
    "    f = filter_size\n",
    "    k = nb_kernel\n",
    "    p = padding\n",
    "    s = stride\n",
    "\n",
    "    # Note the last layer has only 2 outcome as we are not doing a softmax. This needs to be changed once it will be implemented.\n",
    "    for l in range(1, L+1):\n",
    "        if l != L:\n",
    "            if architecture[l] != 'fc':\n",
    "                n_h = int((n_h + 2*p[l] - f[l]) / s[l] + 1)\n",
    "                n_w = int((n_w + 2*p[l] - f[l]) / s[l] + 1)\n",
    "                layer_shape.append((k[l], n_h, n_w, m))\n",
    "            else:\n",
    "                layer_shape.append((k[l], m))\n",
    "        else:\n",
    "            layer_shape.append((2, m))\n",
    "\n",
    "    return layer_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(3, 32, 32, 10),\n (8, 28, 28, 10),\n (8, 14, 14, 10),\n (16, 10, 10, 10),\n (16, 5, 5, 10),\n (120, 10),\n (84, 10),\n (2, 10)]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test get_layer_shape\n",
    "layer_shape = get_layer_shape(architecture, filter_size, nb_kernel, padding, stride, A)\n",
    "\n",
    "layer_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters size:\n",
    "# If Conv/Pool: (curr channel, prev channel, filter, filter) and (cur channel, 1, 1, 1)\n",
    "# If FC from Conv/Pool: (curr channel, vectorized[prev output])\n",
    "# If FC from FC: (curr channel, prev channel)\n",
    "# bias is always (curr channel, 1) when FC\n",
    "def initialize_model(architecture, filter_size, nb_kernel, padding, stride, A):\n",
    "\n",
    "    W = {}; b = {}; Z = {}; dA = {}; dZ = {}\n",
    "    L = len(architecture)\n",
    "    f = filter_size\n",
    "    k = nb_kernel\n",
    "    m = A['0'].shape[3]\n",
    "    layer_shape = get_layer_shape(architecture, filter_size, nb_kernel, padding, stride, A)\n",
    "\n",
    "    for l in range(1, L):\n",
    "\n",
    "        # We need to initialize output as well to allocate the convolution output per index (otherwise Python doesn't allow allocation)\n",
    "        Z[str(l)] = np.zeros((layer_shape[l]))\n",
    "        A[str(l)] = np.zeros((layer_shape[l]))\n",
    "\n",
    "        # Now initializing parameters\n",
    "        if architecture[l] == 'conv':\n",
    "            \n",
    "            W[str(l)] = np.random.randn(k[l], k[l-1], f[l], f[l]) * 0.01\n",
    "            b[str(l)] = np.zeros((k[l], 1, 1, 1))\n",
    "            dZ[str(l)] = np.zeros((layer_shape[l]))\n",
    "            dA[str(l)] = np.zeros((layer_shape[l]))\n",
    "\n",
    "        # Note that there is no parameter, nor Z for Pooling layers\n",
    "        elif (architecture[l] == 'maxpool') | (architecture[l] == 'avgpool'):\n",
    "            W[str(l)] = 0\n",
    "            b[str(l)] = 0\n",
    "            Z[str(l)] = 0\n",
    "            dZ[str(l)] = 0\n",
    "            dA[str(l)] = np.zeros((layer_shape[l-1]))\n",
    "\n",
    "            # We specifically need to raise error if pooling layer kernel is different from convolutional layer kernel\n",
    "            if k[l] != k[l-1]:\n",
    "                raise ValueError(\"Pooling layer kernel # needs to be equal to Convolutional layer kernel #\")\n",
    "\n",
    "        elif architecture[l] =='fc':\n",
    "            # Parameters size different at the moment of change from conv to fc due to vectorized output\n",
    "            if architecture[l-1] != 'fc':\n",
    "                W[str(l)] = np.random.randn(k[l], int(np.prod(A[str(l-1)].shape) / m))\n",
    "            else:\n",
    "                W[str(l)] = np.random.randn(k[l], k[l-1])\n",
    "            \n",
    "            b[str(l)] = np.zeros((k[l], 1))\n",
    "            dZ[str(l)] = np.zeros((layer_shape[l]))\n",
    "            dA[str(l)] = np.zeros((layer_shape[l]))\n",
    "\n",
    "    return A, Z, W, b, dA, dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "init\nA[0] shape: (3, 32, 32, 10)\nconv\nA[1] shape: (8, 28, 28, 10)\nZ[1] shape: (8, 28, 28, 10)\nW[1] shape: (8, 3, 5, 5)\nb[1] shape: (8, 1, 1, 1)\ndA[1] shape: (8, 28, 28, 10)\ndZ[1] shape: (8, 28, 28, 10)\nmaxpool\nA[2] shape: (8, 14, 14, 10)\nZ[2] shape: 0\nW[2] shape: 0\nb[2] shape: 0\ndA[2] shape: (8, 28, 28, 10)\ndZ[2] shape: 0\nconv\nA[3] shape: (16, 10, 10, 10)\nZ[3] shape: (16, 10, 10, 10)\nW[3] shape: (16, 8, 5, 5)\nb[3] shape: (16, 1, 1, 1)\ndA[3] shape: (16, 10, 10, 10)\ndZ[3] shape: (16, 10, 10, 10)\navgpool\nA[4] shape: (16, 5, 5, 10)\nZ[4] shape: 0\nW[4] shape: 0\nb[4] shape: 0\ndA[4] shape: (16, 10, 10, 10)\ndZ[4] shape: 0\nfc\nA[5] shape: (120, 10)\nZ[5] shape: (120, 10)\nW[5] shape: (120, 400)\nb[5] shape: (120, 1)\ndA[5] shape: (120, 10)\ndZ[5] shape: (120, 10)\nfc\nA[6] shape: (84, 10)\nZ[6] shape: (84, 10)\nW[6] shape: (84, 120)\nb[6] shape: (84, 1)\ndA[6] shape: (84, 10)\ndZ[6] shape: (84, 10)\n"
    }
   ],
   "source": [
    "# Test initialize_model\n",
    "A, Z, W, b, dA, dZ = initialize_model(architecture, filter_size, nb_kernel, padding, stride, A)\n",
    "\n",
    "for l in range(L):\n",
    "    print(architecture[l])\n",
    "    print(\"A[{0}] shape: {1}\".format(l, A[str(l)].shape))\n",
    "    if l != 0:\n",
    "        if (architecture[l] == 'maxpool') | (architecture[l] == 'avgpool'):\n",
    "            print(\"Z[{0}] shape: {1}\".format(l, Z[str(l)]))\n",
    "            print(\"W[{0}] shape: {1}\".format(l, W[str(l)]))\n",
    "            print(\"b[{0}] shape: {1}\".format(l, b[str(l)]))\n",
    "            print(\"dA[{0}] shape: {1}\".format(l, dA[str(l)].shape))\n",
    "            print(\"dZ[{0}] shape: {1}\".format(l, dZ[str(l)]))              \n",
    "        else:\n",
    "            print(\"Z[{0}] shape: {1}\".format(l, Z[str(l)].shape))\n",
    "            print(\"W[{0}] shape: {1}\".format(l, W[str(l)].shape))\n",
    "            print(\"b[{0}] shape: {1}\".format(l, b[str(l)].shape))\n",
    "            print(\"dA[{0}] shape: {1}\".format(l, dA[str(l)].shape))\n",
    "            print(\"dZ[{0}] shape: {1}\".format(l, dZ[str(l)].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## III. Creating helper functions\n",
    "___\n",
    "Padding  \n",
    "Slicing  \n",
    "Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We won't be using numpy's pad function as this one is enough and quick to operate\n",
    "# Only works for 3D matrix\n",
    "def add_padding(A, padding, value=0, axis=(0, 1, 2)):\n",
    "\n",
    "    kernel, height, width = axis\n",
    "\n",
    "    horizontal_pad = np.zeros((A.shape[kernel], padding, A.shape[width] + 2*padding)) + value\n",
    "    vertical_pad = np.zeros((A.shape[kernel], A.shape[height], padding)) + value\n",
    "\n",
    "    padded_A = np.concatenate((vertical_pad, A), axis=width)\n",
    "    padded_A = np.concatenate((padded_A, vertical_pad), axis=width)\n",
    "    padded_A = np.concatenate((horizontal_pad, padded_A), axis=height)\n",
    "    padded_A = np.concatenate((padded_A, horizontal_pad), axis=height)\n",
    "\n",
    "    return padded_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Padding: 1\nOriginal matrix shape: (3, 32, 32)\nPadded matrix: (3, 34, 34)\n___________________\nOriginal matrix: [[[0.80910971 0.87126192 0.9479845  ... 0.49261322 0.74834998 0.88788139]\n  [0.50733089 0.9281072  0.40057399 ... 0.84011627 0.54032842 0.50940143]\n  [0.57508981 0.86727363 0.80406552 ... 0.60838744 0.39553176 0.97301442]\n  ...\n  [0.30617513 0.06138716 0.38945002 ... 0.93768146 0.07970471 0.07837573]\n  [0.26788262 0.43128458 0.00665561 ... 0.56605573 0.48859255 0.88772732]\n  [0.08624652 0.37492544 0.73017502 ... 0.32011413 0.25328062 0.45150791]]\n\n [[0.1372327  0.68208641 0.66545846 ... 0.19449202 0.32870502 0.19413085]\n  [0.847629   0.13602974 0.86673516 ... 0.10573889 0.66740911 0.16302562]\n  [0.25002816 0.388352   0.93204938 ... 0.16045953 0.1397784  0.91925739]\n  ...\n  [0.05903123 0.56758681 0.38333621 ... 0.96678757 0.49682607 0.05100984]\n  [0.8637013  0.39219656 0.64871522 ... 0.54660719 0.53349473 0.45051365]\n  [0.72738251 0.70959769 0.54415291 ... 0.10348158 0.93052842 0.31409791]]\n\n [[0.76816931 0.28432268 0.34362477 ... 0.37691113 0.53585646 0.06954268]\n  [0.1386467  0.83632195 0.42498335 ... 0.51378864 0.41845598 0.38797311]\n  [0.80255919 0.54370158 0.39830507 ... 0.81616182 0.99609917 0.11151553]\n  ...\n  [0.27374392 0.2460384  0.02519245 ... 0.1768426  0.32907867 0.58203518]\n  [0.99471496 0.16722449 0.81139998 ... 0.12295208 0.28235237 0.49666334]\n  [0.74126054 0.75573405 0.70068699 ... 0.32347607 0.73033055 0.01726622]]]\n___________________\nPadded matrix: [[[0.         0.         0.         ... 0.         0.         0.        ]\n  [0.         0.80910971 0.87126192 ... 0.74834998 0.88788139 0.        ]\n  [0.         0.50733089 0.9281072  ... 0.54032842 0.50940143 0.        ]\n  ...\n  [0.         0.26788262 0.43128458 ... 0.48859255 0.88772732 0.        ]\n  [0.         0.08624652 0.37492544 ... 0.25328062 0.45150791 0.        ]\n  [0.         0.         0.         ... 0.         0.         0.        ]]\n\n [[0.         0.         0.         ... 0.         0.         0.        ]\n  [0.         0.1372327  0.68208641 ... 0.32870502 0.19413085 0.        ]\n  [0.         0.847629   0.13602974 ... 0.66740911 0.16302562 0.        ]\n  ...\n  [0.         0.8637013  0.39219656 ... 0.53349473 0.45051365 0.        ]\n  [0.         0.72738251 0.70959769 ... 0.93052842 0.31409791 0.        ]\n  [0.         0.         0.         ... 0.         0.         0.        ]]\n\n [[0.         0.         0.         ... 0.         0.         0.        ]\n  [0.         0.76816931 0.28432268 ... 0.53585646 0.06954268 0.        ]\n  [0.         0.1386467  0.83632195 ... 0.41845598 0.38797311 0.        ]\n  ...\n  [0.         0.99471496 0.16722449 ... 0.28235237 0.49666334 0.        ]\n  [0.         0.74126054 0.75573405 ... 0.73033055 0.01726622 0.        ]\n  [0.         0.         0.         ... 0.         0.         0.        ]]]\n"
    }
   ],
   "source": [
    "# Test add_padding\n",
    "pad = 1\n",
    "padded_matrix = add_padding(A['0'][:,:,:,0], pad, value=0, axis=(0, 1, 2))\n",
    "\n",
    "print(\"Padding:\", pad)\n",
    "print(\"Original matrix shape:\", A['0'][:,:,:,0].shape)\n",
    "print(\"Padded matrix:\", padded_matrix.shape)\n",
    "print(\"___________________\")\n",
    "print(\"Original matrix:\", A['0'][:,:,:,0])\n",
    "print(\"___________________\")\n",
    "print(\"Padded matrix:\", padded_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this only works for a 4D matrix\n",
    "def slicing(A, layer, architecture, filter_size, padding, stride, side='fwd'):\n",
    "\n",
    "    if side == 'fwd':\n",
    "        ref = layer - 1\n",
    "    elif side == 'back':\n",
    "        ref = layer\n",
    "    # Padding the matrix to work the slicing on\n",
    "    # This is base matrix where we are going to create all the necessary slicing (hence the need to pad it first)\n",
    "    # Also note that we don't need to specify the value of kernel, as we won't slice through the depth (instead we take it all)\n",
    "    # It takes as input a 4D matrix (for all observation), but return slice for only one observation\n",
    "    f = filter_size\n",
    "    s = stride\n",
    "    kernel, height, width, observation = A[str(layer)].shape\n",
    "\n",
    "    for m in range(observation):\n",
    "        padded_A = add_padding(A[str(ref)][:, :, :, m], padding[layer], value=0, axis=(0, 1, 2))\n",
    "            \n",
    "        for i in range(height):\n",
    "            v1 = i * s[layer]\n",
    "            v2 = i * s[layer] + f[layer]\n",
    "\n",
    "            for j in range(width):\n",
    "                h1 = j * s[layer]\n",
    "                h2 = j * s[layer] + f[layer]\n",
    "\n",
    "                # If convolutional layer, we need to return a fxfxkernel 3D matrix\n",
    "                if architecture[layer] == 'conv':\n",
    "                    slice = padded_A[:, v1:v2, h1:h2]\n",
    "                    # We don't exactly need k, but we still return it to yield the exact number of output in both cases\n",
    "                    k = 0\n",
    "\n",
    "                    yield slice, i, j, k, m\n",
    "\n",
    "                # If pooling layer, we need an output slice of a fxf 2D matrix for each l-1 kernel (we have set kernel to be (l) given that (l-1) = (l) kernel)\n",
    "                elif (architecture[layer] == 'maxpool') | (architecture[layer] == 'avgpool'):\n",
    "                    for k in range(kernel):\n",
    "                        slice = padded_A[k, v1:v2, h1:h2]\n",
    "\n",
    "                        yield slice, i, j, k, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 0 0 0 (2, 2) [[0. 0.]\n [0. 0.]]\n0 1 0 0 (2, 2) [[0. 0.]\n [0. 0.]]\n1 0 0 0 (2, 2) [[0. 0.]\n [0. 0.]]\n1 1 0 0 (2, 2) [[0. 0.]\n [0. 0.]]\n\n___________________\nCurrent layer architecture: maxpool\n# of slices: 15680\nTheoretical # of slices: 15680\nTheoretical dimension of slice: (2, 2)\nReal dimenion of slice: (2, 2)\n"
    }
   ],
   "source": [
    "# Test slicing\n",
    "layer = 2; cnt = 0; dim_check = 0\n",
    "\n",
    "for slice, i, j, k, m in slicing(A, layer, architecture, filter_size, padding, stride, 'fwd'):\n",
    "    if (i < 2) & (j < 2) & (k==0) & (m==0):\n",
    "        print(i, j, k, m, slice.shape, slice)\n",
    "    cnt += 1\n",
    "\n",
    "if architecture[layer] == 'conv':\n",
    "    dim_check = np.prod(A[str(layer)].shape) / A[str(layer)].shape[0]\n",
    "    theoretical_shape = (nb_kernel[layer-1], filter_size[layer], filter_size[layer])\n",
    "elif (architecture[layer] == 'maxpool') | (architecture[layer] == 'avgpool'):\n",
    "    dim_check = np.prod(A[str(layer)].shape)\n",
    "    theoretical_shape = (filter_size[layer], filter_size[layer])\n",
    "\n",
    "print(\"\\n___________________\")\n",
    "print(\"Current layer architecture:\", architecture[layer])\n",
    "print(\"# of slices:\", cnt)\n",
    "print(\"Theoretical # of slices:\", dim_check)\n",
    "print(\"Theoretical dimension of slice:\", theoretical_shape)\n",
    "print(\"Real dimenion of slice:\", slice.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for vectorization / devectorization\n",
    "def vectorization(X, architecture, layer, type='forward'):\n",
    "\n",
    "    # Vectorization\n",
    "    if type == 'forward':\n",
    "        if (architecture[layer] == 'fc') & (architecture[layer-1] != 'fc'):\n",
    "            # When the layer # is 'Fully Connected', we need to vectorize the previous output\n",
    "            # or pass the previous output if it is not 'fc' as no need to do any vectorization\n",
    "            # This does not work for Z, W, or b with the very first hidden layer (only A) as they do not have any value assigned \n",
    "            dim_1, dim_2, dim_3, dim_4 = X[str(layer-1)].shape\n",
    "            new_X = X[str(layer-1)].reshape(dim_1*dim_2*dim_3, dim_4)\n",
    "        else:\n",
    "            new_X = X[str(layer-1)]\n",
    "    \n",
    "    # Devectorization\n",
    "    elif type == 'inverted':\n",
    "        if (architecture[layer+1] == 'fc') & (architecture[layer] == 'maxpool') | (architecture[layer] == 'avgpool'):\n",
    "            new_X = X[str(layer+1)].reshape(X[str(layer-1)].shape)\n",
    "        elif (architecture[layer+1] == 'fc') & (architecture[layer] == 'conv'):\n",
    "            new_X = X[str(layer+1)].reshape(X[str(layer)].shape)\n",
    "        else:\n",
    "            new_X = X[str(layer)]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Type unknown: must be 'forward' or 'inverted'\")\n",
    "\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Layer: 1 conv\n(8, 28, 28, 10)\nLayer: 2 maxpool\n(8, 14, 14, 10)\nLayer: 3 conv\n(16, 10, 10, 10)\nLayer: 4 avgpool\n(16, 5, 5, 10)\nLayer: 5 fc\n(120, 10)\nLayer: 6 fc\n(84, 10)\n\n_____________\nconv maxpool\nVectorized shape: (8, 28, 28, 10)\nExpected shape: (8, 28, 28, 10)\n\n_____________\nmaxpool conv\nDevectorized shape: (16, 10, 10, 10)\nExpected shape: (16, 10, 10, 10)\n\n_____________\navgpool fc\nVectorized shape: (400, 10)\nExpected shape: (400, 10)\n\n_____________\navgpool fc\nDevectorized shape: (120, 10)\nExpected shape: (16, 5, 5, 10)\n"
    }
   ],
   "source": [
    "# Test vectorization function\n",
    "for l in range(1, L):\n",
    "    print(\"Layer:\", l, architecture[l])\n",
    "    print(A[str(l)].shape)\n",
    "\n",
    "print(\"\\n_____________\")\n",
    "print(architecture[1], architecture[2])\n",
    "print(\"Vectorized shape:\", vectorization(A, architecture, 2, 'forward').shape)\n",
    "print(\"Expected shape:\", A['1'].shape)\n",
    "\n",
    "print(\"\\n_____________\")\n",
    "print(architecture[2], architecture[3])\n",
    "print(\"Devectorized shape:\", vectorization(A, architecture, 3, 'inverted').shape)\n",
    "print(\"Expected shape:\", A['3'].shape)\n",
    "\n",
    "print(\"\\n_____________\")\n",
    "print(architecture[4], architecture[5])\n",
    "print(\"Vectorized shape:\", vectorization(A, architecture, 5, 'forward').shape)\n",
    "print(\"Expected shape:\", (A['4'].shape[0]*A['4'].shape[1]*A['4'].shape[2], A['4'].shape[3]))\n",
    "\n",
    "print(\"\\n_____________\")\n",
    "print(architecture[4], architecture[5])\n",
    "print(\"Devectorized shape:\", vectorization(A, architecture, 5, 'inverted').shape)\n",
    "print(\"Expected shape:\", A['4'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IV. Convolutional Forward Propagation\n",
    "___\n",
    "Convolutional Layer:\n",
    "$$ Z ^{[l]} _{(c, i, j, m)} = \\sum _{k} ^{K} \\sum _{h=1} ^{f^{[l]}_h} \\sum_{w=1} ^{f^{[l]}_w} W^{[l]} _{(c, k, h, w)} \\times \\operatorname{pad} (A^{[l-1]} _{(k, s(i-1)+h, s(j-1)+w)}, m) + b^{[l]} _{(c)} \\tag{1} $$  \n",
    "$$ A^{[l]} _{(c, i, j, m)} = g(Z ^{[l]} _{(c, i, j, m)}) \\tag{2} $$  \n",
    "With: $i = h^{[l]} = \\lfloor \\frac {h^{[l-1]} + 2p^{[l]} - f^{[l]} _h} {s^{[l]}} \\rfloor + 1 $, $j = w^{[l]} = \\lfloor \\frac {w^{[l-1]} + 2p^{[l]} - f^{[l]} _w} {s^{[l]}} \\rfloor + 1 $  \n",
    "\n",
    "Here, $h^{[l]}$ and $w^{[l]}$ (therefore i and j) are the height and width of layer \\[l], $p^{[l]}$ is the padding added to the output \\[l-1], $s^{[l]}$ is the stride layer l, c is the channel of layer \\[l], K is the channel of layer \\[l-1], $f^{[l]}_h$ and $f^{[l]}_w$ the height and width of the filter of layer \\[l], and lastly m the number of observation.    \n",
    "Example, i = 1, j = 2...\n",
    "\n",
    "Pooling is to reduce size, and makes the computation easier. There is also an L2 pooling in order to control overfitting.  \n",
    "Max Pooling Layer:  \n",
    "$$ A ^{[l]} _{(c, i, j, m)} = \\max {( A^{[l-1]} _{(c, h_{1}:h_{2}, w_{1}:w_{2}, m)} )} \\tag{3} $$  \n",
    "Average Pooling Layer:  \n",
    "$$ A ^{[l]} _{(c, i, j, m)} = \\frac {1} {f^{[l]}_h \\times f^{[l]}_w} \\times \\sum _{h=1} ^{f^{[l]}_h} \\sum_{w=1} ^{f^{[l]}_w} A^{[l-1]} _{(s(i-1)+h, s(j-1)+w, m)} \\tag{4} $$    \n",
    "\n",
    "With: $h_{1} = s(i-1) + 1$, $h_{2} = s(i-1) + f^{[l]}_h$, $w_{1} = s(j-1) + 1$, $w_{2} = s(j-1) + f^{[l]}_w$, i and j is calculated the same way as convolutional layer. In this layer, there is no $Z^{[l]}$, and the number of channel is equivalent to the previous layer.  \n",
    "\n",
    "Vectorization to Fully connected Layer:  \n",
    "In order to pass the output to the fully connected layer, we need to reduce the dimension e.g. vectorize the output so that it becomes from size (k, h, w, m) to (k*h*w, m), 4D ==> 2D. We will call the vectorization function as f, such as:  \n",
    "$$ f(A^{[l]} _{(k^{[l]}, h^{[l]}, w^{[l]}, m)}) = A^{[l]} _{(k^{[l]} \\times h^{[l]} \\times w^{[l]}, m)} \\tag{5} $$  \n",
    "\n",
    "Fully connected Layer:  \n",
    "Once the vectorization is done, we can then compute the output of the fully connected layer (e.g. traditional Neural Network calculation):  \n",
    "$$Z^{[l]} = W^{[l]} \\times f(A^{[l-1]}) + b^{[l]} \\tag{6} $$\n",
    "$$A^{[l]} = g(Z^{[l]}) \\tag{7} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_forward_propagation(A, Z, architecture, activations, filter_size, nb_kernel, padding, stride):\n",
    "    \n",
    "    L = len(architecture)\n",
    "\n",
    "    # Through each layer\n",
    "    for l in range(1, L):\n",
    "            \n",
    "        # We operate the convolution nb kernel time (corresponding to the output layer kernel, or the current layer filter kernel size)\n",
    "        if architecture[l] == 'conv':\n",
    "            # As mentionned in slicing, loop over kernel is different for pooling and conv (pooling is already in slicing, vs conv is looping output kernel size time)\n",
    "            for k in range(A[str(l)].shape[0]):\n",
    "                for slice, i, j, _, m in slicing(A, l, architecture, filter_size, padding, stride, 'fwd'):\n",
    "                    # We can see that the formula is highly similar to the usual forward propagation, the only diff is that it is an element-wise\n",
    "                    # multiplication instead of a matrix multiplication, and we should return a scalar only\n",
    "                    Z[str(l)][k, i, j, m] = np.sum(np.multiply(W[str(l)][k], slice) + b[str(l)][k])\n",
    "                    A[str(l)][k, i, j, m] = activation_function(activations[l], Z[str(l)][k, i, j, m])\n",
    "\n",
    "        elif architecture[l] == 'maxpool':\n",
    "            for slice, i, j, k, m in slicing(A, l, architecture, filter_size, padding, stride, 'fwd'):\n",
    "                A[str(l)][k, i, j, m] = np.max(slice)\n",
    "\n",
    "        elif architecture[l] == 'avgpool':\n",
    "            for slice, i, j, k, m in slicing(A, l, architecture, filter_size, padding, stride, 'fwd'):\n",
    "                A[str(l)][k, i, j, m] = np.mean(slice)\n",
    "\n",
    "        elif architecture[l] == 'fc':\n",
    "            Z[str(l)] = np.dot(W[str(l)], vectorization(A, architecture, l, type='forward')) + b[str(l)]\n",
    "            A[str(l)] = activation_function(activations[l], Z[str(l)])\n",
    "\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Layer 0, Shape (3, 32, 32, 10)\nLayer 1, Shape (8, 28, 28, 10)\nLayer 2, Shape (8, 14, 14, 10)\nLayer 3, Shape (16, 10, 10, 10)\nLayer 4, Shape (16, 5, 5, 10)\nLayer 5, Shape (120, 10)\nLayer 6, Shape (84, 10)\nLast layer value: [[0.46667822 0.47875382 0.5122367  0.44255163 0.481632   0.48072479\n  0.46234001 0.47405187 0.46649445 0.45243979]\n [0.54567069 0.53869714 0.52471174 0.61638047 0.52244765 0.56533697\n  0.56989171 0.58758283 0.5363253  0.5815607 ]\n [0.45981101 0.44111496 0.43210486 0.45670838 0.44044764 0.44088354\n  0.44108746 0.45655059 0.45829139 0.42018165]\n [0.62575553 0.62493445 0.65338135 0.58879335 0.61693255 0.63863689\n  0.6437032  0.59910572 0.62176423 0.6301343 ]\n [0.49032712 0.51932903 0.52331193 0.50839228 0.53384832 0.45934043\n  0.54134594 0.52826419 0.52438411 0.51228038]\n [0.39589493 0.37729544 0.4002846  0.43858137 0.41653248 0.41374909\n  0.37677974 0.3827707  0.38849233 0.3790446 ]\n [0.34007048 0.32775338 0.32862267 0.3493407  0.34374731 0.33459917\n  0.3191107  0.36800978 0.30887441 0.32130324]\n [0.4523571  0.44436406 0.46599393 0.43471092 0.44730422 0.45765311\n  0.43919834 0.44927988 0.4542168  0.46192144]\n [0.50356713 0.51966575 0.51901271 0.48341397 0.52307827 0.5658154\n  0.50299164 0.49091322 0.48673435 0.5308341 ]\n [0.4610769  0.42751609 0.44897375 0.47270534 0.50845405 0.44348453\n  0.40555961 0.46410502 0.45812571 0.46901123]\n [0.34159096 0.35438077 0.34096298 0.3374889  0.35061946 0.3743488\n  0.35241265 0.33460111 0.32870893 0.36758802]\n [0.61950502 0.60713815 0.62339478 0.62014045 0.60301125 0.61907372\n  0.60965519 0.61411362 0.63899103 0.58920676]\n [0.42907686 0.41208326 0.42509299 0.42026504 0.44559259 0.3772749\n  0.42970424 0.42319711 0.42664531 0.41658468]\n [0.61403358 0.65474086 0.63101039 0.63670961 0.6684728  0.60236047\n  0.64431775 0.62211335 0.63473843 0.63855933]\n [0.44766982 0.42110203 0.41451286 0.37362696 0.41591102 0.44043897\n  0.37571794 0.44047489 0.38373647 0.38892671]\n [0.51669261 0.52714053 0.55441921 0.46399472 0.51009953 0.55496204\n  0.53515402 0.51021851 0.52225959 0.50224054]\n [0.52200737 0.51111845 0.51542749 0.52665189 0.56736553 0.51365833\n  0.53209562 0.49447312 0.54852093 0.52939093]\n [0.47284157 0.4550222  0.5177521  0.47347311 0.44649813 0.47397769\n  0.45602444 0.45972406 0.44334096 0.51107876]\n [0.53560993 0.57534339 0.56438799 0.58348005 0.55910398 0.53526635\n  0.579801   0.58079118 0.56498133 0.55190599]\n [0.42311582 0.42016759 0.46280525 0.43372774 0.45689356 0.41085583\n  0.35355323 0.42063924 0.41732063 0.42436174]\n [0.50540181 0.53928142 0.52926932 0.51019867 0.53648277 0.55006815\n  0.58084212 0.56756404 0.55068443 0.50969617]\n [0.44490472 0.4424004  0.46063763 0.45053129 0.45678721 0.45286922\n  0.43560401 0.47188216 0.4631587  0.46528353]\n [0.43068953 0.48222743 0.46527207 0.46911141 0.43485834 0.48350449\n  0.49578942 0.43620799 0.42657062 0.49066149]\n [0.4796683  0.51733543 0.51406112 0.4731908  0.47459874 0.50293562\n  0.57868547 0.5025768  0.47655619 0.51875791]\n [0.57291045 0.54106507 0.57260731 0.5285699  0.55964557 0.53865755\n  0.51504704 0.51755864 0.53986743 0.53074328]\n [0.28926021 0.30934298 0.29810884 0.34051447 0.35006356 0.31610205\n  0.29919078 0.32926817 0.34041473 0.34784736]\n [0.56633722 0.57185227 0.58560397 0.57050949 0.59625495 0.57314705\n  0.61245508 0.55550612 0.58838713 0.56213982]\n [0.68058689 0.67354436 0.68897218 0.66877746 0.66810526 0.68796661\n  0.7183082  0.64705152 0.64722081 0.67091659]\n [0.42391186 0.37105592 0.41520424 0.36457726 0.41531489 0.33138806\n  0.41084168 0.38861622 0.419265   0.36687758]\n [0.42078975 0.40139469 0.47370258 0.40191803 0.45987019 0.44492115\n  0.4463302  0.44047731 0.42318513 0.47905112]\n [0.56876904 0.5359844  0.56041574 0.52965666 0.55954226 0.56415813\n  0.57894224 0.55649626 0.56990164 0.55161529]\n [0.63814307 0.63234027 0.63670474 0.61040874 0.61422646 0.62179665\n  0.65073507 0.61640311 0.64404388 0.60646095]\n [0.4458612  0.41330987 0.4640269  0.41257061 0.45712229 0.42372013\n  0.39432686 0.42106516 0.43843645 0.38237965]\n [0.57141721 0.61540114 0.6115039  0.56425399 0.56283446 0.57289746\n  0.56361257 0.566534   0.61778421 0.53346203]\n [0.54693524 0.54458723 0.53054899 0.5473975  0.55588936 0.57201254\n  0.54769568 0.53186972 0.55313007 0.55115867]\n [0.43217534 0.42934811 0.40632815 0.41103534 0.40627769 0.39523538\n  0.41274437 0.44059712 0.4058125  0.42665753]\n [0.58814775 0.64243785 0.60692041 0.64664067 0.61444802 0.58874293\n  0.64404598 0.66041089 0.62147484 0.62848412]\n [0.46538649 0.43988075 0.45651011 0.43022524 0.39320962 0.44850848\n  0.44414566 0.42065416 0.44182922 0.43771532]\n [0.61212725 0.61110649 0.60604521 0.63275866 0.62670512 0.63173421\n  0.61634293 0.6208205  0.61430863 0.6564964 ]\n [0.55537862 0.53699994 0.52847042 0.53330421 0.53220836 0.54549171\n  0.5592279  0.55613144 0.55215264 0.5666125 ]\n [0.44020151 0.45151022 0.44217042 0.40808649 0.43619727 0.40982263\n  0.45687124 0.43049537 0.41074688 0.45783113]\n [0.45680077 0.4300087  0.43318085 0.42443446 0.4623379  0.46600141\n  0.43030179 0.47665464 0.45664284 0.41964678]\n [0.49992545 0.44741685 0.44418799 0.46571231 0.43967992 0.43350663\n  0.46048518 0.50094195 0.45936889 0.46909117]\n [0.45544281 0.45203019 0.44265778 0.47515737 0.45610535 0.4849037\n  0.48839258 0.46582101 0.48079965 0.51956307]\n [0.52310025 0.56226862 0.53603912 0.58466948 0.54882123 0.54709159\n  0.55436035 0.56970614 0.53649868 0.49986589]\n [0.39926663 0.42183246 0.39058387 0.41989707 0.43891073 0.42007229\n  0.37136887 0.40043869 0.40876222 0.41773121]\n [0.50394918 0.46261677 0.48489519 0.45069392 0.47845381 0.4238475\n  0.38483881 0.44725846 0.4954251  0.38180499]\n [0.56757561 0.54538619 0.506872   0.52772997 0.50787647 0.50123939\n  0.51436741 0.53153154 0.55201628 0.50136405]\n [0.63006746 0.61194554 0.60603516 0.61209957 0.59241168 0.61235574\n  0.60929285 0.61089645 0.58716655 0.64155247]\n [0.44788127 0.45397614 0.44638882 0.47963432 0.48740133 0.43073785\n  0.42976686 0.49637241 0.45243861 0.47291755]\n [0.53768826 0.53718368 0.48945573 0.54591157 0.55750282 0.55832133\n  0.51413654 0.5494605  0.52171637 0.5252845 ]\n [0.54035625 0.52600096 0.54291525 0.49243716 0.58806301 0.51849019\n  0.52082519 0.51982721 0.48309365 0.53091389]\n [0.62673908 0.60839811 0.63231416 0.63102509 0.62691038 0.64820514\n  0.60482755 0.60708038 0.62454442 0.64590292]\n [0.56938104 0.53942508 0.55634232 0.54301449 0.55786278 0.5465077\n  0.56605677 0.53790186 0.52994185 0.53687949]\n [0.4833475  0.48142409 0.46280978 0.48538931 0.48158837 0.51665662\n  0.50580513 0.49645755 0.51871005 0.52041712]\n [0.64414132 0.64403064 0.63389322 0.60260786 0.61242685 0.63203129\n  0.62943329 0.61403737 0.58176647 0.64067855]\n [0.61812484 0.60939825 0.5825532  0.5743688  0.52624441 0.63093142\n  0.59488781 0.57638232 0.58544938 0.59105182]\n [0.42931409 0.40822894 0.3937311  0.42864361 0.41786668 0.38434139\n  0.43502747 0.43939227 0.42410215 0.42820178]\n [0.51316783 0.51069705 0.50588748 0.49706404 0.53419652 0.51413193\n  0.51322794 0.53197411 0.47365903 0.50739649]\n [0.63599108 0.60576219 0.59899916 0.63237331 0.62779535 0.63921262\n  0.64280534 0.62329354 0.60217589 0.63642468]\n [0.69896561 0.70739483 0.69556125 0.70492033 0.67765054 0.71233461\n  0.72010908 0.68726241 0.70633108 0.69601109]\n [0.62176178 0.58831482 0.62621516 0.57172941 0.58360382 0.60414239\n  0.56626383 0.56120031 0.56949021 0.5496372 ]\n [0.68089149 0.68642939 0.67275798 0.70151473 0.66672087 0.68529562\n  0.66150698 0.66705303 0.65322004 0.62944856]\n [0.43806009 0.44754545 0.46803069 0.46240724 0.47302235 0.41753311\n  0.42243532 0.44955469 0.44890635 0.44156295]\n [0.62430905 0.64801661 0.64216968 0.67932663 0.62814373 0.66335871\n  0.66097586 0.65377725 0.6694702  0.68361756]\n [0.60353198 0.56517491 0.59725665 0.60035607 0.62372014 0.57032356\n  0.57148387 0.56767256 0.57883384 0.56112318]\n [0.60618714 0.63494826 0.61134608 0.62374131 0.60249819 0.66445184\n  0.66584569 0.61035413 0.63237609 0.66383425]\n [0.43602808 0.46651296 0.41919141 0.5205597  0.43980795 0.48778529\n  0.43268766 0.45690534 0.43089326 0.4365836 ]\n [0.32440209 0.35876224 0.34967029 0.39832699 0.37220092 0.38879696\n  0.38620442 0.38532241 0.38161208 0.36564105]\n [0.31695229 0.39588285 0.37480751 0.40414034 0.34504231 0.33899944\n  0.35683765 0.37302672 0.38958954 0.37704396]\n [0.43714666 0.43892883 0.42734815 0.48620015 0.44817765 0.50009668\n  0.40634404 0.46414829 0.42639487 0.46988768]\n [0.3670577  0.44337368 0.4160387  0.38306787 0.40617725 0.40690618\n  0.35684603 0.39606001 0.36999305 0.38950706]\n [0.34408625 0.41251887 0.38946708 0.4409628  0.39687453 0.39124286\n  0.40167478 0.40790426 0.40798538 0.3881376 ]\n [0.47339788 0.42659478 0.45946428 0.40122158 0.45004547 0.44794336\n  0.44911688 0.43240012 0.44829726 0.43402184]\n [0.54954368 0.50101595 0.56518533 0.54496634 0.51348426 0.5329378\n  0.54995271 0.52751831 0.48172855 0.54971996]\n [0.6246769  0.63754586 0.59640341 0.63025076 0.59087652 0.64137836\n  0.65202293 0.62696823 0.62606915 0.6583096 ]\n [0.47248799 0.51980796 0.50813247 0.49462039 0.49427357 0.52666277\n  0.51928737 0.49892267 0.51911336 0.51645284]\n [0.42377112 0.43119261 0.45641363 0.46903186 0.47468934 0.43474793\n  0.48195146 0.45408306 0.46291188 0.45029428]\n [0.52041557 0.52906659 0.5727538  0.53273661 0.55918178 0.56008306\n  0.50727972 0.53913742 0.51874919 0.53055763]\n [0.47497811 0.44597192 0.46962658 0.47846911 0.50057786 0.43757268\n  0.43038472 0.45192935 0.43927668 0.47170004]\n [0.60517524 0.62453809 0.64549134 0.63066834 0.61179756 0.64150485\n  0.65536978 0.63565255 0.56022515 0.66281092]\n [0.4029531  0.42580695 0.42640348 0.42509639 0.40808189 0.40633688\n  0.46009543 0.41454514 0.4018262  0.46154863]\n [0.3879937  0.40132805 0.39150878 0.41456159 0.430926   0.35476585\n  0.380328   0.38617166 0.39560153 0.39087563]\n [0.46982547 0.45532861 0.46976264 0.43978984 0.46740814 0.45086209\n  0.44595615 0.46568762 0.4847509  0.44046737]]\nCost: 60.15025886431846\n"
    }
   ],
   "source": [
    "# Test forward prop\n",
    "Z, A = convolutional_forward_propagation(A, Z, architecture, activations, filter_size, nb_kernel, padding, stride)\n",
    "\n",
    "for l in range(L):\n",
    "    print(\"Layer {0}, Shape {1}\".format(l, A[str(l)].shape))\n",
    "\n",
    "print(\"Last layer value:\", A[str(L-1)])\n",
    "print(\"Cost:\", compute_cost(A[str(L-1)], Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## V. Convolutional Backward Propagation\n",
    "___\n",
    "Recall the generalized scheme: Conv ==> Pool ==> Vectorized ==> FC.  \n",
    "Derivation is therefore: FC ==> Devectorized ==> Pool ==> Conv.\n",
    "  \n",
    "Fully connected layer:  \n",
    "We simply reuse the traditional fully connected neural network formulas:\n",
    "$$dA^{[L]} = \\frac{\\partial Cost}{\\partial A^{[L]}} = \\frac{-Y}{A^{[L]}} + \\frac{(1-Y)}{(1-A^{[L]})} \\tag{8}$$\n",
    "$$dZ^{[L]} = \\frac{\\partial Cost}{\\partial A^{[L]}} \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}} = dA^{[L]} * g'(Z^{[L]}) \\tag{9}$$\n",
    "\n",
    "If there would have only one fully connected layer, we would stop at equation (9) and get gradients for parameters W and b.\n",
    "$$dW^{[l]} = \\frac{1}{m} dZ^{[l]} \\times A'^{[l-1]} \\tag{10}$$  \n",
    "$$db^{[l]} = \\frac{1}{m} \\sum \\limits_{i=1}^{m} {dZ^{[l]}} \\tag{11}$$  \n",
    "\n",
    "For more fully connected layers at the end, it goes:\n",
    "$$dA^{[l-1]} = W'^{[l]} \\times dZ^{[l]} \\tag{10}$$\n",
    "$$dZ^{[l-1]} = W'^{[l]} \\times dZ^{[l]} * g'(Z^{[l-1]}) \\tag{11}$$  \n",
    "And we use (10) and (11) for dW, db of layer \\[l-1].  \n",
    "  \n",
    "  \n",
    "Devectorization:  \n",
    "Recall that the first fully connected layer input is the vectorized \\[l-1] output, e.g. assuming the vectorization is done at layer \\[l]: $Z^{[l]} = W^{[l]} \\times f(A^{[l-1]}) + b^{[l]}$. This means that we need to calculate $\\frac{\\partial Cost}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial f(A^{[l-1]})}$ first.  \n",
    "$$ dA^{[l-1]} = \\frac{\\partial Cost}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial f(A^{[l-1]})} = W'^{[l]} \\times dZ^{[l]}$$  \n",
    "And now we can devectorize:  \n",
    "$$ \\operatorname{devectorized} (dA^{[l-1]}) = F^{-1}(\\frac{\\partial Cost}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial f(A^{[l-1]})}) = F^{-1}(W'^{[l]} \\times dZ^{[l]}) \\tag{12} $$\n",
    "Which is of dimension $(k^{[l-1]}, h^{[l-1]}, w^{[l-1]}, m)$ if there is no pooling layer, and $(k^{[l-2]}, h^{[l-2]}, w^{[l-2]}, m)$ if there is a pooling layer (the forward propagation of pooling is done to reduce the size of the matrix, but the backprop returns 0 or average in the original convolution layer matrix, and therefore we never really uses the pooling dimension, which keeps the size of the input for the output as we will see in the next section). We can now pass on the devectorized gradient to the following layers.\n",
    "  \n",
    "Pooling layer:  \n",
    "Pooling layers have no parameters, but we still need to instruct the following layers of the \"winner cells\" to continue the backward propagation process. In each case, we have: $h_{1} = s(i-1) + 1$, $h_{2} = s(i-1) + f^{[l]}_h$, $w_{1} = s(j-1) + 1$, $w_{2} = s(j-1) + f^{[l]}_w$, i and j is calculated the same way as convolutional layer, and dimension of the output is the same as the dimension of the input.  \n",
    "\n",
    "Max Pooling:\n",
    "For Max pooling, winner cells are the one which are the maximum. The rest are equals to 0. Therefore:  \n",
    "$$ dA^{[l]} _{(c, h_{1}:h_{2}, w_{1}:w_{2}, m)} = \\frac{\\partial Cost}{\\partial f(A^{[l]})} \\frac{\\partial f(A^{[l]})}{\\partial A^{[l]}} = \\operatorname{arg\\,max} (\\operatorname{devectorized} (dA^{[l]} _{(c, h_{1}:h_{2}, w_{1}:w_{2}, m)}) ) \\tag{13} $$\n",
    "\n",
    "Average Pooling:\n",
    "Reversing the average means we allocate back to each cell the value of the average. Therefore each cell receive the value of $\\frac {average} {\\#cells}$, i.e.:\n",
    "$$ dA^{[l]} _{(c, h_{1}:h_{2}, w_{1}:w_{2}, m)} = \\frac{\\partial Cost}{\\partial f(A^{[l]})} \\frac{\\partial f(A^{[l]})}{\\partial A^{[l]}} = \\frac {1} {f^{[l]}_h \\times f^{[l]}_w} \\times \\operatorname{devectorized} (dA^{[l]} _{(c, h_{1}:h_{2}, w_{1}:w_{2}, m)}) \\tag{14} $$  \n",
    "  \n",
    "Convolutional layer:  \n",
    "We can now continue the derivation following the chain rule:  \n",
    "$$ dZ^{[l]} = \\frac{\\partial Cost}{\\partial A^{[l]}} \\frac{\\partial A^{[l]}} {\\partial Z^{[l]}} = dA^{[l]} * g'(Z^{[l]}) $$\n",
    "With $ dA^{[l]} $ being the pooling gradient matrix, of dimension $A^{[l]conv]}$ given we use the Convolutional layer output as input for pooling backpropagation.\n",
    "Given equation (1), it is easy to derive dW as: \n",
    "$$ dW^{[l]} _{(c, k, h, w)} = \\frac{\\partial Cost}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}} {\\partial W^{[l]} _{(k, h, w)}} = \\sum _{i=1} ^{n^{[l]}_H} \\sum_{j=1} ^ {n^{[l]}_W} dZ ^{[l]} _{(c, i, j, m)} \\times \\operatorname{pad} (A^{[l-1]} _{(k, s(i-1)+h, s(j-1)+w, m)}) \\tag{15}$$\n",
    "\n",
    "Indeed, for a specific filter c, the value of the kth kernel ith row and jth column of the gradients of W is a specific value of the previous output, that we have many times because of the parameter sharing specificity of convolution. We do use this specific value of $W^{[l]}_{(c, k, i, j)}$ $n^{[l]}_H \\times n^{[l]}_W$ times, and this for each $dZ^{[l]} _{(c, i, j, m)}$. Note also that $k \\in [1, K^{[l]}]$, $i \\in [1, f^{[l]}_h]$, $j \\in [1, f^{[l]}_w]$.  \n",
    "The same reasoning can be applied to compute $db^{[l]}$, which is given by:\n",
    "$$ db^{[l]} _{(c)} = \\frac{\\partial Cost}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}} {\\partial b^{[l]} _{(c)}} = \\sum _{i=1} ^{n^{[l]}_H} \\sum_{j=1} ^ {n^{[l]}_W} dZ ^{[l]} _{(c, i, j, m)} \\tag{16}$$\n",
    "\n",
    "Note that if the next layer is also a convolutional layer, e.g. an architecture of conv -> conv, the calculation of $dA^{[l]}$ becomes:\n",
    "$$ dA^{[l-1]} _{(k, s(i-1)+h, s(j-1)+w, m)} = \\frac{\\partial Cost}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}} {\\partial A^{[l-1]} _{(k, s(i-1)+h, s(j-1)+w, m)}} = \\sum_{c=1} ^ {n^{[l]}_C} \\sum _{i=1} ^{n^{[l]}_H} \\sum_{j=1} ^ {n^{[l]}_W}  W^{[l]} _{(c, k, h, w)} \\times dZ ^{[l]} _{(c, i, j, m)}$$  \n",
    "\n",
    "Simplifying the indexing to: $dA^{[l-1]} _{(k, i', j', m)}$, which means $i' = s(i-1) + h$ and $j' = s(i-1) + w$, we find that $i = \\frac {i'-h} {s} + 1$, $j = \\frac {j'-w} {s} + 1$. However, i and j cannot be below 1 as they correspond to the minimum index of the output matrix $dZ^{[l]}$, and it also cannot go beyond its size. In other words, i and j are both floored at 1 and are capped at $n^{[l]}_H$ and $n^{[l]}_W$ respectively. Therefore, $\\forall h \\in [1, f^{[l]}_h]$, $\\forall w \\in [1, f^{[l]}_w]$:  \n",
    "$$ dA^{[l-1]} _{(k, i', j', m)} = \\frac{\\partial Cost}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}} {\\partial A^{[l-1]} _{(k, i', j', m)}} = \\sum_{c=1} ^ {n^{[l]}_C} \\sum _{i=\\max (1, \\frac {i'-h} {s} + 1)} ^{\\min (i', n^{[l]}_H)} \\sum_{j= \\max (1, \\frac {j'-w} {s} + 1)} ^ {\\min (j', n^{[l]}_W)} W^{[l]} _{(c, k, h, w)} \\times dZ ^{[l]} _{(c, i, j, m)} \\tag{17} $$  \n",
    "\n",
    "We finally unpad $dA^{[l-1]} _{(k, i', j', m)}$ by the appropriate padding value to come back to the original input matrix size, so that we can carry calculating gradients through (14), (15) and (16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_backward_propagation(A, Z, dA, dZ, architecture, filter_size, padding, stride):\n",
    "\n",
    "    # Works similar to normal backprop for fully connected layer\n",
    "    # We need to compute the derivative of the last layer separately, as it is a non-generic formula\n",
    "    # 1e-8 is added for numeric stability\n",
    "\n",
    "    L = len(architecture)-1; dW = {}; db = {}\n",
    "    s = stride\n",
    "    f = filter_size\n",
    "    m = A['0'].shape[3]\n",
    "\n",
    "    dAL = -Y / (A[str(L)] + 1e-8) + (1-Y) / ((1-A[str(L)]) + 1e-8)\n",
    "    dZ[str(L)] = dAL * backward_activation_function(activations[L], Z[str(L)])\n",
    "    dW[str(L)] = 1/m * np.dot(dZ[str(L)], A[str(L-1)].T)\n",
    "    db[str(L)] = 1/m * np.sum(dZ[str(L)], axis=1, keepdims=True)\n",
    "\n",
    "    # We can now compute the generalized backward propagation\n",
    "    for l in range(L-1, 0, -1):\n",
    "\n",
    "        # We use the same code for fully connected layer\n",
    "        if architecture[l] == 'fc':\n",
    "            dZ[str(l)] = np.dot(W[str(l+1)].T, dZ[str(l+1)]) * backward_activation_function(activations[l], Z[str(l)])\n",
    "            dW[str(l)] = 1/m * np.dot(dZ[str(l)], vectorization(A, architecture, l, 'forward').T)\n",
    "            db[str(l)] = 1/m * np.sum(dZ[str(l)], axis=1, keepdims=True)    \n",
    "        \n",
    "        elif architecture[l] == 'maxpool':\n",
    "            # Going backward through a maximum function, it is argmax function. So for the max slice part, it returns 0 for all-non max values\n",
    "            # When we just follow a fully connected layer, we need to \"devectorized\", e.g. reshape the layer into the original matrix dimension\n",
    "            for slice, i, j, k, o in slicing(vectorization(A, architecture, l, 'inverted'), l, architecture, filter_size, padding, stride, 'fwd'):\n",
    "                v1 = i * s[l]\n",
    "                v2 = i * s[l] + f[l]\n",
    "                h1 = j * s[l]\n",
    "                h2 = j * s[l] + f[l]\n",
    "\n",
    "                # The way it works: we take the previous layer output A[l-1], we slice through (the same as forward prop)\n",
    "                # We get the maximum number in that slice, we keep only this number and the rest should be 0\n",
    "                # We call this slice therefore a mask, as it transforms the original output A[l-1] into dZ[l]\n",
    "                # Once we get one mask, we add to dZ[l] which only contains 0 at the moment. We go through the next mask and we add it once again\n",
    "                # And so it goes\n",
    "                mask = (slice == np.max(slice))\n",
    "                dA[str(l)][k, v1:v2, h1:h2, o] += mask * slice\n",
    "\n",
    "        elif architecture[l] == 'avgpool':\n",
    "            for slice, i, j, k, o in slicing(A, l, architecture, filter_size, padding, stride, 'fwd'):\n",
    "                v1 = i * s[l]\n",
    "                v2 = i * s[l] + f[l]\n",
    "                h1 = j * s[l]\n",
    "                h2 = j * s[l] + f[l]\n",
    "\n",
    "                # The way it works: we take the previous layer output A[l-1], we slice through (the same as forward prop)\n",
    "                # We get the maximum number in that slice, we keep only this number and the rest should be 0\n",
    "                # We call this slice therefore a mask, as it transforms the original output A[l-1] into dZ[l]\n",
    "                # Once we get one mask, we add to dZ[l] which only contains 0 at the moment. We go through the next mask and we add it once again\n",
    "                # And so it goes\n",
    "                dA[str(l)][k, v1:v2, h1:h2, o] = np.mean(slice) / np.dot(slice.shape)\n",
    "\n",
    "        elif architecture[l] == 'conv':\n",
    "            print(\"A faire\")           \n",
    "\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw, db = convolutional_backward_propagation(A, Z, dA, dZ, architecture, filter_size, padding, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(k, v1, v2, h1, h2, o, dA[str(l)].shape, dA[str(l)][k, v1:v2, h1:h2, o].shape, slice.shape, A[str(l-1)].shape)\n",
    "np.sum(dA['2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = update_parameters(len(architecture), W, b, dW, db, learning_rate=0.01)\n",
    "\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VI. LeNet-5 Full Model\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}