{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bita6faa0f504cb47b9b202f4621b79fec8",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "___\n",
    "# __Convolutional Neural Network from scratch__\n",
    "### _Author: Aki Taniguchi_\n",
    "### _Original date: 19/02/2020_\n",
    "### _Last update: 28/02/2020_\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I. Setting environment\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:\\\\Users\\\\tngch\\\\Python Code and AI\\\\Neural Network\")\n",
    "\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from Deep_Neural_Network import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(3, 32, 32, 10)\n",
    "Y = np.random.choice(2, 10)\n",
    "\n",
    "A = {}\n",
    "A['0'] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the hyper-parameters (\"LeNet-5\")\n",
    "architecture = ['init', 'conv', 'maxpool', 'conv', 'maxpool', 'fc', 'fc']\n",
    "activations = ['init', 'relu', 'relu', 'relu', 'relu', 'relu', 'sigmoid'] # Pooling layer has no activation, but we need to specify the same as convolutional layer for backpropagation calculation purposes\n",
    "filter_size = [0, 5, 2, 5, 2, 1, 1]\n",
    "nb_kernel = [3, 8, 8, 16, 16, 120, 84] # first is RGB, fully connected layers are the number of neurons\n",
    "padding = [0, 0, 0, 0, 0, 0, 0]\n",
    "stride = [0, 1, 2, 1, 2, 0, 0]\n",
    "L = len(architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## II. Initialize model\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Shape of Output A. Note that the layer with full connection needs to be vectorized, but won't be here\n",
    "# dim A is (channel, height, width, observation)\n",
    "# width and height are both determined: int[(x + 2p - f) / s + 1]\n",
    "def get_layer_shape(architecture, filter_size, nb_kernel, padding, stride, A):\n",
    "\n",
    "    layer_shape = [A['0'].shape]\n",
    "\n",
    "    m = A['0'].shape[3]\n",
    "    n_h = A['0'].shape[1]\n",
    "    n_w = A['0'].shape[2]\n",
    "    \n",
    "    L = len(architecture)\n",
    "    f = filter_size\n",
    "    k = nb_kernel\n",
    "    p = padding\n",
    "    s = stride\n",
    "\n",
    "    # Note the last layer has only 2 outcome as we are not doing a softmax. This needs to be changed once it will be implemented.\n",
    "    for l in range(1, L+1):\n",
    "        if l != L:\n",
    "            if architecture[l] != 'fc':\n",
    "                n_h = int((n_h + 2*p[l] - f[l]) / s[l] + 1)\n",
    "                n_w = int((n_w + 2*p[l] - f[l]) / s[l] + 1)\n",
    "                layer_shape.append((k[l], n_h, n_w, m))\n",
    "            else:\n",
    "                layer_shape.append((k[l], m))\n",
    "        else:\n",
    "            layer_shape.append((2, m))\n",
    "\n",
    "    return layer_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(3, 32, 32, 10),\n (8, 28, 28, 10),\n (8, 14, 14, 10),\n (16, 10, 10, 10),\n (16, 5, 5, 10),\n (120, 10),\n (84, 10),\n (2, 10)]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test get_layer_shape\n",
    "layer_shape = get_layer_shape(architecture, filter_size, nb_kernel, padding, stride, A)\n",
    "\n",
    "layer_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters size:\n",
    "# If Conv/Pool: (curr channel, prev channel, filter, filter) and (cur channel, 1, 1, 1)\n",
    "# If FC from Conv/Pool: (curr channel, vectorized[prev output])\n",
    "# If FC from FC: (curr channel, prev channel)\n",
    "# bias is always (curr channel, 1) when FC\n",
    "def initialize_model(architecture, filter_size, nb_kernel, padding, stride, A):\n",
    "\n",
    "    W = {}; b = {}; Z = {}; dA = {}; dZ = {}; dW = {}; db = {}\n",
    "    L = len(architecture)\n",
    "    f = filter_size\n",
    "    k = nb_kernel\n",
    "    m = A['0'].shape[3]\n",
    "    layer_shape = get_layer_shape(architecture, filter_size, nb_kernel, padding, stride, A)\n",
    "\n",
    "    for l in range(1, L):\n",
    "\n",
    "        # We need to initialize output as well to allocate the convolution output per index (otherwise Python doesn't allow allocation)\n",
    "        A[str(l)] = np.zeros((layer_shape[l]))\n",
    "        dA[str(l)] = np.zeros((layer_shape[l]))\n",
    "\n",
    "        # Now initializing parameters\n",
    "        if architecture[l] == 'conv':\n",
    "            W[str(l)] = np.random.randn(k[l], k[l-1], f[l], f[l]) * 0.01\n",
    "            b[str(l)] = np.zeros((k[l], 1, 1, 1))\n",
    "            Z[str(l)] = np.zeros((layer_shape[l]))\n",
    "            dZ[str(l)] = np.zeros((layer_shape[l]))\n",
    "\n",
    "        # Note that there is no parameter for Pooling layers (there is no Z neither, but we need to keep it in order to compute the generalized backpropagation)\n",
    "        elif (architecture[l] == 'maxpool') | (architecture[l] == 'avgpool'):\n",
    "            W[str(l)] = 0\n",
    "            b[str(l)] = 0\n",
    "            Z[str(l)] = 0\n",
    "            dZ[str(l)] = 0\n",
    "\n",
    "            # We specifically need to raise error if pooling layer kernel is different from convolutional layer kernel\n",
    "            if k[l] != k[l-1]:\n",
    "                raise ValueError(\"Pooling layer kernel # needs to be equal to Convolutional layer kernel #\")\n",
    "\n",
    "        elif architecture[l] =='fc':\n",
    "            # Parameters size different at the moment of change from conv to fc due to vectorized output\n",
    "            Z[str(l)] = np.zeros((layer_shape[l]))\n",
    "            dZ[str(l)] = np.zeros((layer_shape[l]))\n",
    "            \n",
    "            if architecture[l-1] != 'fc':\n",
    "                W[str(l)] = np.random.randn(k[l], int(np.prod(A[str(l-1)].shape) / m))\n",
    "                dA[str(l-1)] = np.zeros((int(np.prod(A[str(l-1)].shape) / m), m))\n",
    "            \n",
    "            else:\n",
    "                W[str(l)] = np.random.randn(k[l], k[l-1])\n",
    "            \n",
    "            b[str(l)] = np.zeros((k[l], 1))\n",
    "        \n",
    "        dW[str(l)] = W[str(l)]\n",
    "        db[str(l)] = b[str(l)]\n",
    "\n",
    "    return A, Z, W, b, dA, dZ, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "init\nA[0] shape: (3, 32, 32, 10)\nconv\nA[1] shape: (8, 28, 28, 10)\nZ[1] shape: (8, 28, 28, 10)\nW[1] shape: (8, 3, 5, 5)\nb[1] shape: (8, 1, 1, 1)\ndW[1] shape: (8, 3, 5, 5)\ndb[1] shape: (8, 1, 1, 1)\ndZ[1] shape: (8, 28, 28, 10)\ndA[1] shape: (8, 28, 28, 10)\nmaxpool\nA[2] shape: (8, 14, 14, 10)\nZ[2] shape: 0\nW[2] shape: 0\nb[2] shape: 0\ndW[2] shape: 0\ndb[2] shape: 0\ndZ[2] shape: 0\ndA[2] shape: (8, 14, 14, 10)\nconv\nA[3] shape: (16, 10, 10, 10)\nZ[3] shape: (16, 10, 10, 10)\nW[3] shape: (16, 8, 5, 5)\nb[3] shape: (16, 1, 1, 1)\ndW[3] shape: (16, 8, 5, 5)\ndb[3] shape: (16, 1, 1, 1)\ndZ[3] shape: (16, 10, 10, 10)\ndA[3] shape: (16, 10, 10, 10)\nmaxpool\nA[4] shape: (16, 5, 5, 10)\nZ[4] shape: 0\nW[4] shape: 0\nb[4] shape: 0\ndW[4] shape: 0\ndb[4] shape: 0\ndZ[4] shape: 0\ndA[4] shape: (400, 10)\nfc\nA[5] shape: (120, 10)\nZ[5] shape: (120, 10)\nW[5] shape: (120, 400)\nb[5] shape: (120, 1)\ndW[5] shape: (120, 400)\ndb[5] shape: (120, 1)\ndZ[5] shape: (120, 10)\ndA[5] shape: (120, 10)\nfc\nA[6] shape: (84, 10)\nZ[6] shape: (84, 10)\nW[6] shape: (84, 120)\nb[6] shape: (84, 1)\ndW[6] shape: (84, 120)\ndb[6] shape: (84, 1)\ndZ[6] shape: (84, 10)\ndA[6] shape: (84, 10)\n"
    }
   ],
   "source": [
    "# Test initialize_model\n",
    "A, Z, W, b, dA, dZ, dW, db = initialize_model(architecture, filter_size, nb_kernel, padding, stride, A)\n",
    "\n",
    "for l in range(L):\n",
    "    print(architecture[l])\n",
    "    print(\"A[{0}] shape: {1}\".format(l, A[str(l)].shape))\n",
    "    if l != 0:\n",
    "        if (architecture[l] == 'maxpool') | (architecture[l] == 'avgpool'):\n",
    "            print(\"Z[{0}] shape: {1}\".format(l, Z[str(l)]))\n",
    "            print(\"W[{0}] shape: {1}\".format(l, W[str(l)]))\n",
    "            print(\"b[{0}] shape: {1}\".format(l, b[str(l)]))\n",
    "            print(\"dW[{0}] shape: {1}\".format(l, dW[str(l)]))\n",
    "            print(\"db[{0}] shape: {1}\".format(l, db[str(l)])) \n",
    "            print(\"dZ[{0}] shape: {1}\".format(l, dZ[str(l)]))         \n",
    "        else:\n",
    "            print(\"Z[{0}] shape: {1}\".format(l, Z[str(l)].shape))\n",
    "            print(\"W[{0}] shape: {1}\".format(l, W[str(l)].shape))\n",
    "            print(\"b[{0}] shape: {1}\".format(l, b[str(l)].shape))\n",
    "            print(\"dW[{0}] shape: {1}\".format(l, dW[str(l)].shape))\n",
    "            print(\"db[{0}] shape: {1}\".format(l, db[str(l)].shape)) \n",
    "            print(\"dZ[{0}] shape: {1}\".format(l, dZ[str(l)].shape))\n",
    "        \n",
    "        print(\"dA[{0}] shape: {1}\".format(l, dA[str(l)].shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## III. Creating helper functions\n",
    "___\n",
    "Padding  \n",
    "Slicing  \n",
    "Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We won't be using numpy's pad function as this one is enough and quick to operate\n",
    "# Only works for 3D matrix\n",
    "def add_padding(A, padding, value=0, axis=(0, 1, 2)):\n",
    "\n",
    "    kernel, height, width = axis\n",
    "\n",
    "    horizontal_pad = np.zeros((A.shape[kernel], padding, A.shape[width] + 2*padding)) + value\n",
    "    vertical_pad = np.zeros((A.shape[kernel], A.shape[height], padding)) + value\n",
    "\n",
    "    padded_A = np.concatenate((vertical_pad, A), axis=width)\n",
    "    padded_A = np.concatenate((padded_A, vertical_pad), axis=width)\n",
    "    padded_A = np.concatenate((horizontal_pad, padded_A), axis=height)\n",
    "    padded_A = np.concatenate((padded_A, horizontal_pad), axis=height)\n",
    "\n",
    "    return padded_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Padding: 1\nOriginal matrix shape: (3, 32, 32)\nPadded matrix: (3, 34, 34)\n___________________\nOriginal matrix: [[[0.37151965 0.07025497 0.31861885 ... 0.72898616 0.02767313 0.13290114]\n  [0.62471469 0.77584161 0.05978884 ... 0.75453208 0.49078958 0.99886057]\n  [0.0302718  0.61844605 0.11690156 ... 0.10171411 0.14334798 0.25977845]\n  ...\n  [0.0042432  0.08342377 0.79400455 ... 0.98278525 0.17494768 0.80045494]\n  [0.19861717 0.02868305 0.54998394 ... 0.18053927 0.24176602 0.15441161]\n  [0.21969244 0.84510994 0.95508207 ... 0.92476469 0.52470126 0.37176531]]\n\n [[0.09710171 0.47013497 0.72139299 ... 0.75536227 0.05757846 0.89362501]\n  [0.98098903 0.48172223 0.52671822 ... 0.39910069 0.6457488  0.61975799]\n  [0.26151976 0.80411867 0.49233817 ... 0.02958888 0.93669982 0.29381764]\n  ...\n  [0.79067032 0.41422091 0.17277068 ... 0.17401158 0.1624773  0.37149736]\n  [0.30883119 0.06104547 0.17586591 ... 0.4912697  0.48825867 0.74583437]\n  [0.59259736 0.80382458 0.22061806 ... 0.85656882 0.87032323 0.70365785]]\n\n [[0.49489885 0.75231509 0.02986233 ... 0.11423054 0.86889643 0.32058442]\n  [0.27328495 0.58479878 0.73264171 ... 0.06992908 0.60445436 0.84193201]\n  [0.97856782 0.85078079 0.45218676 ... 0.24287727 0.48790723 0.26498957]\n  ...\n  [0.07620941 0.04213808 0.95437126 ... 0.9279615  0.32410251 0.10412278]\n  [0.33796994 0.01160403 0.30003592 ... 0.5881329  0.59325105 0.15450276]\n  [0.20601911 0.68230689 0.31285144 ... 0.67175066 0.13550397 0.89251854]]]\n___________________\nPadded matrix: [[[0.         0.         0.         ... 0.         0.         0.        ]\n  [0.         0.37151965 0.07025497 ... 0.02767313 0.13290114 0.        ]\n  [0.         0.62471469 0.77584161 ... 0.49078958 0.99886057 0.        ]\n  ...\n  [0.         0.19861717 0.02868305 ... 0.24176602 0.15441161 0.        ]\n  [0.         0.21969244 0.84510994 ... 0.52470126 0.37176531 0.        ]\n  [0.         0.         0.         ... 0.         0.         0.        ]]\n\n [[0.         0.         0.         ... 0.         0.         0.        ]\n  [0.         0.09710171 0.47013497 ... 0.05757846 0.89362501 0.        ]\n  [0.         0.98098903 0.48172223 ... 0.6457488  0.61975799 0.        ]\n  ...\n  [0.         0.30883119 0.06104547 ... 0.48825867 0.74583437 0.        ]\n  [0.         0.59259736 0.80382458 ... 0.87032323 0.70365785 0.        ]\n  [0.         0.         0.         ... 0.         0.         0.        ]]\n\n [[0.         0.         0.         ... 0.         0.         0.        ]\n  [0.         0.49489885 0.75231509 ... 0.86889643 0.32058442 0.        ]\n  [0.         0.27328495 0.58479878 ... 0.60445436 0.84193201 0.        ]\n  ...\n  [0.         0.33796994 0.01160403 ... 0.59325105 0.15450276 0.        ]\n  [0.         0.20601911 0.68230689 ... 0.13550397 0.89251854 0.        ]\n  [0.         0.         0.         ... 0.         0.         0.        ]]]\n"
    }
   ],
   "source": [
    "# Test add_padding\n",
    "pad = 1\n",
    "padded_matrix = add_padding(A['0'][:,:,:,0], pad, value=0, axis=(0, 1, 2))\n",
    "\n",
    "print(\"Padding:\", pad)\n",
    "print(\"Original matrix shape:\", A['0'][:,:,:,0].shape)\n",
    "print(\"Padded matrix:\", padded_matrix.shape)\n",
    "print(\"___________________\")\n",
    "print(\"Original matrix:\", A['0'][:,:,:,0])\n",
    "print(\"___________________\")\n",
    "print(\"Padded matrix:\", padded_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for vectorization / devectorization\n",
    "def vectorization(original_matrix, matrix_dict, architecture, layer, type='vector'):\n",
    "\n",
    "    # Vectorization\n",
    "    if type == 'vector':\n",
    "        if (architecture[layer] == 'fc') & (architecture[layer-1] != 'fc'):\n",
    "            # When the layer # is 'Fully Connected', we need to vectorize the previous output\n",
    "            # or pass the previous output if it is not 'fc' as no need to do any vectorization\n",
    "            # This does not work for Z, W, or b with the very first hidden layer (only A) as they do not have any value assigned \n",
    "            dim_1, dim_2, dim_3, dim_4 = matrix_dict[str(layer-1)].shape\n",
    "            new_val = original_matrix.reshape(dim_1*dim_2*dim_3, dim_4)\n",
    "        else:\n",
    "            new_val = original_matrix\n",
    "\n",
    "    # Devectorization\n",
    "    elif type == 'matrix':\n",
    "        if (architecture[layer+1] == 'fc') & (architecture[layer] != 'fc'):\n",
    "            new_val = original_matrix.reshape(matrix_dict[str(layer)].shape)\n",
    "        else:\n",
    "            new_val = original_matrix\n",
    "    else:\n",
    "        raise ValueError(\"Type unknown: must be 'vector' or 'matrix'\")\n",
    "\n",
    "    return new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Layer: 1 conv\nLayer: 2 maxpool\nLayer: 3 conv\nLayer: 4 maxpool\nLayer: 5 fc\nLayer: 6 fc\n\n_____________\nmaxpool fc\nFwd Original shape: (16, 5, 5, 10)\nVectorized shape: (400, 10)\n\n_____________\nmaxpool conv\nBckwd Original shape: (400, 10)\nDevectorized shape: (16, 5, 5, 10)\n\n_____________\nconv maxpool\nFwd Original shape: (8, 28, 28, 10)\nVectorized shape: (8, 28, 28, 10)\n\n_____________\nfc maxpool\nBckwd Original shape: (120, 10)\nDevectorized shape: (120, 10)\n"
    }
   ],
   "source": [
    "# Test vectorization function\n",
    "for l in range(1, L):\n",
    "    print(\"Layer:\", l, architecture[l])\n",
    "\n",
    "# Vectorization test function\n",
    "def vec_test(l, type):\n",
    "    print(\"\\n_____________\")\n",
    "    if type == 'vector':\n",
    "        print(architecture[l-1], architecture[l])\n",
    "        print('Fwd Original shape:', A[str(l-1)].shape)\n",
    "        print(\"Vectorized shape:\", vectorization(A[str(l-1)], A, architecture, l, type).shape)\n",
    "    if type == 'matrix':\n",
    "        print(architecture[l+1], architecture[l])\n",
    "        print('Bckwd Original shape:', dA[str(l+1)].shape)\n",
    "        print(\"Devectorized shape:\", vectorization(dA[str(l+1)], A, architecture, l+1, type).shape)\n",
    "\n",
    "# Case where we need to vectorize. We are in the \"fully connected layer\" (l=5), and we use vectorization on the previous layer to do the forward propagation\n",
    "vec_test(5, 'vector')\n",
    "\n",
    "# Case where we need to devectorize. We are in the layer before the \"fully connected\" layer (e.g. one step after the fully connected layer backward, l=4), but we need to devectorize the \"fully connected\" layer output gradient to keep going backward\n",
    "vec_test(3, 'matrix')\n",
    "\n",
    "# We now make sure that no vectorization happens when it is not necessary. Let us test vectorization on layer 2 and layer 4.\n",
    "vec_test(2, 'vector')\n",
    "vec_test(4, 'matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this only works for a 4D matrix\n",
    "def slicing(A, dA, layer, architecture, filter_size, padding, stride, type='forward'):\n",
    "\n",
    "    # Padding the matrix to work the slicing on\n",
    "    # This is base matrix where we are going to create all the necessary slicing (hence the need to pad it first)\n",
    "    # Also note that we don't need to specify the value of kernel, as we won't slice through the depth (instead we take it all)\n",
    "    # It takes as input a 4D matrix (for all observation), but return slice for only one observation\n",
    "    f = filter_size\n",
    "    s = stride\n",
    "\n",
    "    # For backpropagation, the slice is bound by the dimension of the previous gradient shape\n",
    "    # Whereas forward propagation slice is bound by the dimension of the current output shape\n",
    "    if type == 'forward':\n",
    "        kernel, height, width, observation = A[str(layer)].shape\n",
    "        back = 0\n",
    "    elif type == 'backward':\n",
    "        kernel, height, width, observation = vectorization(A[str(layer+1)], dA, architecture, layer+1, 'matrix').shape\n",
    "        back = 1\n",
    "    else:\n",
    "        raise ValueError(\"Type must be 'forward' or 'backward'\")\n",
    "\n",
    "    for m in range(observation):\n",
    "        padded_A = add_padding(A[str(layer-1 + back)][:, :, :, m], padding[layer], value=0, axis=(0, 1, 2))\n",
    "        # +1 is added for back propagation, as it works on the current gradient\n",
    "        # Forward propagation works on the previous output\n",
    "\n",
    "        for i in range(height):\n",
    "            v1 = i * s[layer]\n",
    "            v2 = i * s[layer] + f[layer]\n",
    "\n",
    "            for j in range(width):\n",
    "                h1 = j * s[layer]\n",
    "                h2 = j * s[layer] + f[layer]\n",
    "\n",
    "                # If convolutional layer, we need to return a fxfxkernel 3D matrix\n",
    "                # Note that backpropagation is one step ahead, therefore we need to add +1 to the index\n",
    "                if architecture[layer + back] == 'conv':\n",
    "                    slice = padded_A[:, v1:v2, h1:h2]\n",
    "                    # We don't exactly need k, but we still return it to yield the exact number of output in both cases\n",
    "                    k = 0\n",
    "\n",
    "                    yield slice, i, j, k, m\n",
    "\n",
    "                # If pooling layer, we need an output slice of a fxf 2D matrix for each l-1 kernel (we have set kernel to be (l) given that (l-1) = (l) kernel)\n",
    "                elif (architecture[layer + back] == 'maxpool') | (architecture[layer + back] == 'avgpool'):\n",
    "                    for k in range(kernel):\n",
    "                        slice = padded_A[k, v1:v2, h1:h2]\n",
    "\n",
    "                        yield slice, i, j, k, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 0 0 0 (2, 2) [[0. 0.]\n [0. 0.]]\n0 1 0 0 (2, 2) [[0. 0.]\n [0. 0.]]\n1 0 0 0 (2, 2) [[0. 0.]\n [0. 0.]]\n1 1 0 0 (2, 2) [[0. 0.]\n [0. 0.]]\n\n___________________\nCurrent layer architecture: maxpool\n# of slices: 15680\nTheoretical # of slices: 15680\nTheoretical dimension of slice: (2, 2)\nReal dimenion of slice: (2, 2)\n"
    }
   ],
   "source": [
    "# Test slicing\n",
    "layer = 2; cnt = 0; dim_check = 0\n",
    "\n",
    "for slice, i, j, k, m in slicing(A, dA, layer, architecture, filter_size, padding, stride, type='forward'):\n",
    "    if (i < 2) & (j < 2) & (k==0) & (m==0):\n",
    "        print(i, j, k, m, slice.shape, slice)\n",
    "    cnt += 1\n",
    "\n",
    "if architecture[layer] == 'conv':\n",
    "    dim_check = np.prod(A[str(layer)].shape) / A[str(layer)].shape[0]\n",
    "    theoretical_shape = (nb_kernel[layer-1], filter_size[layer], filter_size[layer])\n",
    "elif (architecture[layer] == 'maxpool') | (architecture[layer] == 'avgpool'):\n",
    "    dim_check = np.prod(A[str(layer)].shape)\n",
    "    theoretical_shape = (filter_size[layer], filter_size[layer])\n",
    "\n",
    "print(\"\\n___________________\")\n",
    "print(\"Current layer architecture:\", architecture[layer])\n",
    "print(\"# of slices:\", cnt)\n",
    "print(\"Theoretical # of slices:\", dim_check)\n",
    "print(\"Theoretical dimension of slice:\", theoretical_shape)\n",
    "print(\"Real dimenion of slice:\", slice.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IV. Convolutional Forward Propagation\n",
    "___\n",
    "Convolutional Layer:\n",
    "$$ Z ^{[l]} _{(c, i, j, m)} = \\sum _{k} ^{K} \\sum _{h=1} ^{f^{[l]}_h} \\sum_{w=1} ^{f^{[l]}_w} W^{[l]} _{(c, k, h, w)} \\times \\operatorname{pad} (A^{[l-1]} _{(k, s(i-1)+h, s(j-1)+w), m)} + b^{[l]} _{(c)} \\tag{1} $$  \n",
    "$$ A^{[l]} _{(c, i, j, m)} = g(Z ^{[l]} _{(c, i, j, m)}) \\tag{2} $$  \n",
    "With: $i = h^{[l]} = \\lfloor \\frac {h^{[l-1]} + 2p^{[l]} - f^{[l]} _h} {s^{[l]}} \\rfloor + 1 $, $j = w^{[l]} = \\lfloor \\frac {w^{[l-1]} + 2p^{[l]} - f^{[l]} _w} {s^{[l]}} \\rfloor + 1 $  \n",
    "\n",
    "Here, $h^{[l]}$ and $w^{[l]}$ (therefore i and j) are the height and width of layer \\[l], $p^{[l]}$ is the padding added to the output \\[l-1], $s^{[l]}$ is the stride layer l, c is the channel of layer \\[l], K is the channel of layer \\[l-1], $f^{[l]}_h$ and $f^{[l]}_w$ the height and width of the filter of layer \\[l], and lastly m the number of observation.    \n",
    "Example, i = 1, j = 2...\n",
    "\n",
    "Pooling is to reduce size, and makes the computation easier. There is also an L2 pooling in order to control overfitting.  \n",
    "Max Pooling Layer:  \n",
    "$$ A ^{[l]} _{(c, i, j, m)} = \\max {( A^{[l-1]} _{(c, h_{1}:h_{2}, w_{1}:w_{2}, m)} )} \\tag{3} $$  \n",
    "Average Pooling Layer:  \n",
    "$$ A ^{[l]} _{(c, i, j, m)} = \\frac {1} {f^{[l]}_h \\times f^{[l]}_w} \\times \\sum _{h=1} ^{f^{[l]}_h} \\sum_{w=1} ^{f^{[l]}_w} A^{[l-1]} _{(s(i-1)+h, s(j-1)+w, m)} \\tag{4} $$    \n",
    "\n",
    "With: $h_{1} = s(i-1) + 1$, $h_{2} = s(i-1) + f^{[l]}_h$, $w_{1} = s(j-1) + 1$, $w_{2} = s(j-1) + f^{[l]}_w$, i and j is calculated the same way as convolutional layer. In this layer, there is no $Z^{[l]}$, and the number of channel is equivalent to the previous layer.  \n",
    "\n",
    "Vectorization to Fully connected Layer:  \n",
    "In order to pass the output to the fully connected layer, we need to reduce the dimension e.g. vectorize the output so that it becomes from size (k, h, w, m) to (k*h*w, m), 4D ==> 2D. We will call the vectorization function as f, such as:  \n",
    "$$ f(A^{[l]} _{(k^{[l]}, h^{[l]}, w^{[l]}, m)}) = A^{[l]} _{(k^{[l]} \\times h^{[l]} \\times w^{[l]}, m)} \\tag{5} $$  \n",
    "\n",
    "Fully connected Layer:  \n",
    "Once the vectorization is done, we can then compute the output of the fully connected layer (e.g. traditional Neural Network calculation):  \n",
    "$$Z^{[l]} = W^{[l]} \\times f(A^{[l-1]}) + b^{[l]} \\tag{6} $$\n",
    "$$A^{[l]} = g(Z^{[l]}) \\tag{7} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_forward_propagation(A, Z, W, b, architecture, activations, filter_size, nb_kernel, padding, stride):\n",
    "    \n",
    "    L = len(architecture)\n",
    "\n",
    "    # Through each layer\n",
    "    for l in range(1, L):\n",
    "            \n",
    "        # We operate the convolution nb kernel time (corresponding to the output layer kernel, or the current layer filter kernel size)\n",
    "        if architecture[l] == 'conv':\n",
    "            # As mentionned in slicing, loop over kernel is different for pooling and conv (pooling is already in slicing, vs conv is looping output kernel size time)\n",
    "            observation = A[str(l)].shape[0]\n",
    "            for k in range(observation):\n",
    "                for slice, i, j, _, m in slicing(A, dA, l, architecture, filter_size, padding, stride, type='forward'):\n",
    "                    # We can see that the formula is highly similar to the usual forward propagation, the only diff is that it is an element-wise\n",
    "                    # multiplication instead of a matrix multiplication, and we should return a scalar only\n",
    "                    Z[str(l)][k, i, j, m] = np.sum(np.multiply(W[str(l)][k], slice) + b[str(l)][k])\n",
    "                    A[str(l)][k, i, j, m] = activation_function(activations[l], Z[str(l)][k, i, j, m])\n",
    "\n",
    "        elif architecture[l] == 'maxpool':\n",
    "            for slice, i, j, k, m in slicing(A, dA, l, architecture, filter_size, padding, stride, type='forward'):\n",
    "                A[str(l)][k, i, j, m] = np.max(slice)\n",
    "\n",
    "        elif architecture[l] == 'avgpool':\n",
    "            for slice, i, j, k, m in slicing(A, dA, l, architecture, filter_size, padding, stride, type='forward'):\n",
    "                A[str(l)][k, i, j, m] = np.mean(slice)\n",
    "\n",
    "        elif architecture[l] == 'fc':\n",
    "            Z[str(l)] = np.dot(W[str(l)], vectorization(A[str(l-1)], A, architecture, l, type='vector')) + b[str(l)]\n",
    "            A[str(l)] = activation_function(activations[l], Z[str(l)])\n",
    "\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Layer 0, Shape (3, 32, 32, 10)\nLayer 1, Shape (8, 28, 28, 10)\nLayer 2, Shape (8, 14, 14, 10)\nLayer 3, Shape (16, 10, 10, 10)\nLayer 4, Shape (16, 5, 5, 10)\nLayer 5, Shape (120, 10)\nLayer 6, Shape (84, 10)\nLast layer value: [[0.37966676 0.37643932 0.35895938 0.3381942  0.30435635 0.38299862\n  0.3159191  0.3612046  0.35247208 0.3336124 ]\n [0.5490493  0.66401687 0.53471855 0.6238223  0.65146576 0.66551313\n  0.68189881 0.57888066 0.6572134  0.69305806]\n [0.34918772 0.24539039 0.41191404 0.25217365 0.28468065 0.3533122\n  0.3676646  0.34483278 0.26534948 0.30586712]\n [0.70185093 0.73262428 0.63274314 0.610737   0.60904756 0.66787278\n  0.63483771 0.69259392 0.69830401 0.6226203 ]\n [0.43908671 0.36580498 0.38033638 0.35741605 0.46378682 0.3963641\n  0.37060211 0.33772418 0.44423355 0.3551965 ]\n [0.63912454 0.6316727  0.66731639 0.62456743 0.56633069 0.57387684\n  0.61174786 0.64180271 0.55750235 0.5925119 ]\n [0.63910396 0.59167442 0.6202843  0.62769592 0.5889897  0.64148359\n  0.5476277  0.55836115 0.63512489 0.62534388]\n [0.52832122 0.52842797 0.42153167 0.53710968 0.53350289 0.5116432\n  0.54179124 0.47396767 0.51181306 0.56823382]\n [0.60668414 0.5339736  0.62334807 0.62905553 0.62989024 0.58297839\n  0.62339727 0.56102036 0.60084236 0.55206901]\n [0.61962858 0.63004077 0.60668567 0.61002646 0.61435763 0.62315698\n  0.58671772 0.65886882 0.659577   0.5672454 ]\n [0.60655292 0.67743467 0.56299811 0.67968773 0.63860348 0.69103206\n  0.62855464 0.65579405 0.6611187  0.6723311 ]\n [0.62722112 0.53349073 0.58449047 0.59323843 0.56523515 0.55795073\n  0.58022024 0.55701373 0.54260619 0.55350389]\n [0.63129168 0.54538005 0.64986918 0.59549538 0.56941956 0.51269996\n  0.58793394 0.55082362 0.63991089 0.46730493]\n [0.39152585 0.41294438 0.34454517 0.30618637 0.31223098 0.26630453\n  0.36437788 0.41495337 0.38441764 0.38491215]\n [0.38674354 0.31358465 0.33617588 0.42200286 0.38858547 0.4806884\n  0.51799229 0.43782125 0.43231239 0.5122484 ]\n [0.44139913 0.5506094  0.44679796 0.48991682 0.49638426 0.5907825\n  0.44203784 0.47448919 0.61320229 0.47044132]\n [0.35716948 0.22108694 0.31535317 0.24970103 0.29230846 0.23545732\n  0.36435291 0.24293113 0.27110235 0.30287228]\n [0.64594626 0.56834058 0.56702361 0.67811221 0.59712898 0.60682924\n  0.59678819 0.53223452 0.54579379 0.55149161]\n [0.311968   0.31355205 0.38932919 0.30666962 0.32144085 0.30803011\n  0.29246012 0.36100745 0.34932804 0.34615193]\n [0.60108071 0.63412858 0.59184    0.66398171 0.65282005 0.62405646\n  0.62522808 0.61267839 0.69437339 0.64006951]\n [0.3314167  0.3197761  0.37145067 0.3571626  0.3774536  0.3336609\n  0.3569862  0.33704188 0.35850563 0.30319493]\n [0.56026753 0.54643026 0.56623987 0.561454   0.60433199 0.6377299\n  0.61413806 0.58621198 0.59621556 0.54426329]\n [0.4916836  0.5302294  0.41501051 0.5129147  0.49777394 0.45579834\n  0.52622528 0.48039081 0.4411996  0.43839602]\n [0.42174561 0.46297085 0.3315182  0.4004521  0.35611257 0.3740638\n  0.48686856 0.47598816 0.41600162 0.51547605]\n [0.59466105 0.48375855 0.61490191 0.57892002 0.50427055 0.58767719\n  0.45146896 0.60980897 0.47167193 0.56300257]\n [0.63042874 0.72403282 0.6982272  0.61968323 0.62274368 0.61289926\n  0.68008847 0.66440762 0.62766978 0.66423719]\n [0.61823122 0.66149737 0.64983883 0.6045176  0.60208167 0.54071041\n  0.56757256 0.6495046  0.55708541 0.60975007]\n [0.68818183 0.73187785 0.70064646 0.76648804 0.70263525 0.72628738\n  0.6848044  0.73788381 0.71884024 0.70774431]\n [0.55720453 0.55319149 0.62688062 0.54222087 0.57888982 0.5805007\n  0.51008611 0.55397441 0.53957559 0.58361323]\n [0.42641613 0.51835193 0.48340706 0.52651975 0.51401264 0.51722562\n  0.57578598 0.55663239 0.45748362 0.52909718]\n [0.4047005  0.45137323 0.42805773 0.51393605 0.47873933 0.42733317\n  0.44295826 0.39579972 0.49878391 0.47037512]\n [0.25914199 0.26650391 0.25060432 0.27756987 0.26752924 0.27590859\n  0.2732758  0.30789183 0.23123013 0.31692171]\n [0.52492874 0.40953994 0.45076155 0.37364623 0.37860725 0.40608974\n  0.31034882 0.41449242 0.40348037 0.34184591]\n [0.36137339 0.36982181 0.4211702  0.38618372 0.39922849 0.36139201\n  0.49578077 0.35725392 0.35714884 0.43976529]\n [0.59817524 0.64497444 0.7347301  0.58153565 0.63663241 0.6557824\n  0.66917063 0.58964126 0.66317677 0.67957757]\n [0.48675727 0.40746383 0.49676414 0.44111569 0.48046055 0.4851749\n  0.48679143 0.43142174 0.44402521 0.44628843]\n [0.46350075 0.3266264  0.46923719 0.4378272  0.42825022 0.47398096\n  0.47444028 0.42541904 0.47043046 0.39270431]\n [0.3687936  0.34675766 0.38474495 0.40345318 0.37777285 0.34478145\n  0.37001926 0.38792716 0.3475779  0.46831797]\n [0.57106931 0.53054016 0.54302747 0.56209606 0.51555768 0.62102923\n  0.5926008  0.56302033 0.62267818 0.50228762]\n [0.57159483 0.55728164 0.55160251 0.55177582 0.50984222 0.55374834\n  0.51868576 0.50892795 0.52401584 0.48518889]\n [0.38970405 0.36640615 0.50403931 0.46972294 0.42396385 0.38787038\n  0.4634772  0.40729824 0.41612989 0.48765838]\n [0.6952141  0.66717975 0.69287997 0.68267927 0.6528163  0.62054134\n  0.67082058 0.67287562 0.69165385 0.68543166]\n [0.31629124 0.26673754 0.38374611 0.29535919 0.37362606 0.2754061\n  0.32595711 0.35454787 0.32449895 0.28344412]\n [0.50145772 0.49703178 0.5006227  0.41211299 0.42512757 0.44515758\n  0.47554147 0.48607888 0.43606003 0.46316618]\n [0.67405288 0.64344821 0.6708213  0.639399   0.63916882 0.6989732\n  0.57927646 0.67922131 0.67459516 0.65618317]\n [0.17826849 0.19013808 0.19182971 0.13738416 0.18193047 0.14676006\n  0.2029805  0.20734063 0.19036486 0.21488278]\n [0.4384056  0.56148918 0.43417644 0.53586763 0.52626666 0.54196847\n  0.57115302 0.55946971 0.56967336 0.55154005]\n [0.63985264 0.70245758 0.67388698 0.71888141 0.68402098 0.67435745\n  0.70111842 0.65803324 0.72894917 0.63725839]\n [0.63165743 0.55131889 0.62312808 0.50681492 0.54867234 0.56430421\n  0.55915652 0.57873506 0.56099628 0.52015199]\n [0.55127568 0.53689303 0.65998263 0.57812314 0.57599256 0.60931362\n  0.55008003 0.56046347 0.53738423 0.59906339]\n [0.45257761 0.43724816 0.39542816 0.42243133 0.37434013 0.42558304\n  0.48680356 0.49962206 0.41699813 0.42981806]\n [0.51016648 0.55296552 0.46433305 0.51646816 0.50351585 0.60654169\n  0.49028931 0.47447769 0.51654721 0.50895209]\n [0.50665093 0.44153196 0.50903082 0.44913944 0.48994316 0.50988745\n  0.45325442 0.4795459  0.41400886 0.4732257 ]\n [0.55538592 0.49021411 0.49309963 0.53574349 0.58183899 0.52912278\n  0.50756296 0.47418898 0.50598881 0.4994961 ]\n [0.25614274 0.22613099 0.26023424 0.26868109 0.30770195 0.30066894\n  0.27645028 0.25755233 0.28582797 0.33279899]\n [0.36506343 0.40543901 0.41671016 0.44066145 0.49963886 0.41363829\n  0.45969537 0.39601245 0.44231904 0.44119882]\n [0.67485885 0.5582232  0.61168998 0.68436919 0.66712555 0.64094564\n  0.59902039 0.60149317 0.69575822 0.59988178]\n [0.51689779 0.56975148 0.51154964 0.45738741 0.4538877  0.38684207\n  0.4258091  0.52924934 0.45326882 0.49340536]\n [0.5749939  0.60334835 0.66439649 0.61181111 0.6690091  0.61557139\n  0.54664745 0.64962814 0.66132971 0.5658767 ]\n [0.71604786 0.60322883 0.70290314 0.68271063 0.57253635 0.69945431\n  0.68808765 0.6651986  0.62943338 0.65644755]\n [0.59855815 0.60173313 0.59100185 0.49004708 0.52225594 0.52552298\n  0.51535624 0.51963212 0.52761898 0.52634438]\n [0.70425886 0.71726533 0.68898404 0.64104283 0.64549993 0.66407397\n  0.60174778 0.64833893 0.6524838  0.66011262]\n [0.52982166 0.46053826 0.41506583 0.54791624 0.51870429 0.51319482\n  0.52465126 0.51967008 0.4766811  0.51242142]\n [0.417453   0.47003311 0.37897941 0.34952919 0.44486772 0.40366981\n  0.3769687  0.4414025  0.38617607 0.373224  ]\n [0.43310612 0.35632137 0.37984265 0.41123306 0.45655781 0.53344442\n  0.43882712 0.48795202 0.33856032 0.44161593]\n [0.44078809 0.43437038 0.48086495 0.41313045 0.45011281 0.36541618\n  0.41767426 0.38999362 0.46955724 0.42472832]\n [0.53534653 0.48343672 0.48139972 0.52378314 0.47672748 0.54044599\n  0.53565657 0.53383524 0.50991296 0.50096644]\n [0.45895879 0.57170123 0.59273454 0.53640257 0.48933255 0.53357374\n  0.54365929 0.5451396  0.49331404 0.54886953]\n [0.35213773 0.45202635 0.36932789 0.40285336 0.4567915  0.4649095\n  0.47885112 0.44328786 0.3630773  0.52405259]\n [0.51162598 0.57482016 0.54276472 0.56560518 0.53367489 0.57455254\n  0.51010543 0.50475138 0.48771831 0.53010838]\n [0.51715375 0.43362467 0.46466219 0.53695001 0.52858317 0.57469709\n  0.42813004 0.46783519 0.55334063 0.5435728 ]\n [0.64331594 0.55356368 0.63066476 0.63501541 0.52941785 0.60765729\n  0.6543642  0.62979844 0.56485325 0.5588432 ]\n [0.54880236 0.65333727 0.54803475 0.59292354 0.50857798 0.65241109\n  0.49402295 0.51153432 0.62320532 0.58450881]\n [0.22426345 0.27627703 0.23971218 0.32853185 0.38626364 0.20869582\n  0.3398746  0.29356608 0.23373052 0.38628804]\n [0.45368515 0.39770023 0.44162174 0.43491921 0.45000343 0.42041338\n  0.38695906 0.40267766 0.42526819 0.37796543]\n [0.37454411 0.32777997 0.34043496 0.33185736 0.29357304 0.29801797\n  0.32253005 0.35014246 0.33624612 0.29789381]\n [0.490418   0.37782751 0.41432446 0.41551693 0.4486344  0.5086906\n  0.47586983 0.39050135 0.46210115 0.47492767]\n [0.28631443 0.26065407 0.34433569 0.22993838 0.2815108  0.25739713\n  0.2853307  0.30019144 0.2927939  0.33225851]\n [0.46768624 0.48370906 0.57493169 0.48837609 0.50207447 0.50312179\n  0.51338458 0.46507895 0.55269722 0.45481143]\n [0.59598651 0.52801205 0.62701    0.54931105 0.57338873 0.55148198\n  0.55454141 0.58139168 0.63997692 0.62021584]\n [0.63437084 0.66469499 0.69430085 0.65503703 0.68962728 0.6492726\n  0.71338894 0.63480065 0.7069668  0.74642798]\n [0.64921259 0.67657445 0.68244685 0.72256221 0.70451796 0.65224106\n  0.65940175 0.63723517 0.66318881 0.62145346]\n [0.43768277 0.43605185 0.519602   0.37842103 0.40953271 0.47762033\n  0.41781618 0.47716282 0.45166776 0.50349626]\n [0.64217144 0.59955439 0.59705173 0.55128663 0.528247   0.52853557\n  0.5399962  0.58012573 0.58476008 0.50719705]]\nCost: 60.920345596365955\n"
    }
   ],
   "source": [
    "# Test forward prop\n",
    "Z, A = convolutional_forward_propagation(A, Z, W, b, architecture, activations, filter_size, nb_kernel, padding, stride)\n",
    "\n",
    "for l in range(L):\n",
    "    print(\"Layer {0}, Shape {1}\".format(l, A[str(l)].shape))\n",
    "\n",
    "print(\"Last layer value:\", A[str(L-1)])\n",
    "print(\"Cost:\", compute_cost(A[str(L-1)], Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## V. Convolutional Backward Propagation\n",
    "___\n",
    "Recall the generalized scheme: Conv ==> Pool ==> Vectorized ==> FC.  \n",
    "Derivation is therefore: FC ==> Devectorized ==> Pool ==> Conv.\n",
    "  \n",
    "Fully connected layer:  \n",
    "We simply reuse the traditional fully connected neural network formulas:\n",
    "$$dA^{[L]} = \\frac{\\partial Cost}{\\partial A^{[L]}} = \\frac{-Y}{A^{[L]}} + \\frac{(1-Y)}{(1-A^{[L]})} \\tag{8}$$\n",
    "$$dZ^{[L]} = \\frac{\\partial Cost}{\\partial A^{[L]}} \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}} = dA^{[L]} * g'(Z^{[L]}) \\tag{9}$$\n",
    "\n",
    "If there would have only one fully connected layer, we would stop at equation (9) and get gradients for parameters W and b.\n",
    "$$dW^{[l]} = \\frac{1}{m} dZ^{[l]} \\times A'^{[l-1]} \\tag{10}$$  \n",
    "$$db^{[l]} = \\frac{1}{m} \\sum \\limits_{i=1}^{m} {dZ^{[l]}} \\tag{11}$$  \n",
    "\n",
    "For more fully connected layers at the end, it goes:\n",
    "$$dA^{[l-1]} = W'^{[l]} \\times dZ^{[l]} \\tag{10}$$\n",
    "$$dZ^{[l-1]} = W'^{[l]} \\times dZ^{[l]} * g'(Z^{[l-1]}) \\tag{11}$$  \n",
    "And we use (10) and (11) for dW, db of layer \\[l-1].  \n",
    "  \n",
    "  \n",
    "Devectorization:  \n",
    "Recall that the first fully connected layer input is the vectorized \\[l-1] output, e.g. assuming the vectorization is done at layer \\[l]: $Z^{[l]} = W^{[l]} \\times f(A^{[l-1]}) + b^{[l]}$. This means that we need to calculate $\\frac{\\partial Cost}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial f(A^{[l-1]})}$ first.  \n",
    "$$ dA^{[l-1]} = \\frac{\\partial Cost}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial f(A^{[l-1]})} = W'^{[l]} \\times dZ^{[l]}$$  \n",
    "And now we can devectorize:  \n",
    "$$ \\operatorname{devectorized} (dA^{[l-1]}) = F^{-1}(\\frac{\\partial Cost}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial f(A^{[l-1]})}) = F^{-1}(W'^{[l]} \\times dZ^{[l]}) \\tag{12} $$\n",
    "Which is of dimension $(k^{[l-1]}, h^{[l-1]}, w^{[l-1]}, m)$. We can now pass on the devectorized gradient to the following layers.\n",
    "  \n",
    "Pooling layer:  \n",
    "Pooling layers have no parameters, but we still need to instruct the following layers of the \"winner cells\" to continue the backward propagation process. In each case, we have: $h_{1} = s(i-1) + 1$, $h_{2} = s(i-1) + f^{[l]}_h$, $w_{1} = s(j-1) + 1$, $w_{2} = s(j-1) + f^{[l]}_w$, i and j is calculated the same way as convolutional layer, and dimension of the output is the same as the dimension of the input. Note that in both case, $dA^{[l]}$ is initiated with 0, and the value of backpropagation is added to each cell (e.g. we stack up its value when cells are overlapping). \n",
    "\n",
    "Max Pooling:\n",
    "For Max pooling, winner cells are the one which are the maximum of the previous layer's output. The rest are equals to 0. Therefore:  \n",
    "$$ dA^{[l]} _{(c, h, w, m)} = \\frac{\\partial Cost}{\\partial f(A^{[l]})} \\frac{\\partial f(A^{[l]})}{\\partial A^{[l]}} = dA^{[l]} _{(c, h, w, m)} + \\max (\\operatorname{devectorized} (dA^{[l+1]} _{(c, h_{1}:h_{2}, w_{1}:w_{2}, m)}) ) \\tag{13} $$\n",
    "\n",
    "Average Pooling:\n",
    "Reversing the average means we allocate back to each cell the value of the average. Therefore each cell receive the value of $\\frac {average} {\\#cells}$, i.e.:\n",
    "$$ dA^{[l]} _{(c, h, w, m)} = \\frac{\\partial Cost}{\\partial f(A^{[l]})} \\frac{\\partial f(A^{[l]})}{\\partial A^{[l]}} = dA^{[l]} _{(c, h, w, m)} + \\frac {1} {f^{[l]}_h \\times f^{[l]}_w} \\times \\sum_{i=1} ^ {f^{[l]}_h} \\sum_{j=1} ^ {f^{[l]}_w} \\operatorname{devectorized} (dA^{[l+1]} _{(c, i, j, m)}) \\tag{14} $$  \n",
    "  \n",
    "Convolutional layer:  \n",
    "We can now continue the derivation following the chain rule:  \n",
    "$$ dZ^{[l]} = \\frac{\\partial Cost}{\\partial A^{[l]}} \\frac{\\partial A^{[l]}} {\\partial Z^{[l]}} = dA^{[l]} * g'(Z^{[l]}) $$\n",
    "With $ dA^{[l]} $ being the pooling gradient matrix, of dimension $A^{[l]conv]}$ given we use the Convolutional layer output as input for pooling backpropagation.\n",
    "Given equation (1), it is easy to derive dW as: \n",
    "$$ dW^{[l]} _{(c, k, h, w)} = \\frac{\\partial Cost}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}} {\\partial W^{[l]} _{(k, h, w)}} = \\frac {1} {m} \\times \\sum _{i=1} ^{n^{[l]}_H} \\sum_{j=1} ^ {n^{[l]}_W} dZ ^{[l]} _{(c, i, j, m)} \\times \\operatorname{pad} (A^{[l-1]} _{(k, s(i-1)+h, s(j-1)+w, m)}) \\tag{15}$$\n",
    "\n",
    "Indeed, for a specific filter c, the value of the kth kernel ith row and jth column of the gradients of W is a specific value of the previous output, that we have many times because of the parameter sharing specificity of convolution. We do use this specific value of $W^{[l]}_{(c, k, i, j)}$ $n^{[l]}_H \\times n^{[l]}_W$ times, and this for each $dZ^{[l]} _{(c, i, j, m)}$. Note also that $k \\in [1, K^{[l]}]$, $i \\in [1, f^{[l]}_h]$, $j \\in [1, f^{[l]}_w]$.  \n",
    "The same reasoning can be applied to compute $db^{[l]}$, which is given by:\n",
    "$$ db^{[l]} _{(c)} = \\frac{\\partial Cost}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}} {\\partial b^{[l]} _{(c)}} = \\frac {1} {m} \\times \\sum _{i=1} ^{n^{[l]}_H} \\sum_{j=1} ^ {n^{[l]}_W} dZ ^{[l]} _{(c, i, j, m)} \\tag{16}$$\n",
    "\n",
    "Note that if the next layer is also a convolutional layer, e.g. an architecture of conv -> conv, the calculation of $dA^{[l]}$ becomes:\n",
    "$$ dA^{[l-1]} _{(k, s(i-1)+h, s(j-1)+w, m)} = \\frac{\\partial Cost}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}} {\\partial A^{[l-1]} _{(k, s(i-1)+h, s(j-1)+w, m)}} = \\sum_{c=1} ^ {n^{[l]}_C} \\sum _{i=1} ^{n^{[l]}_H} \\sum_{j=1} ^ {n^{[l]}_W}  W^{[l]} _{(c, k, h, w)} \\times dZ ^{[l]} _{(c, i, j, m)}$$  \n",
    "\n",
    "Simplifying the indexing to: $dA^{[l-1]} _{(k, i', j', m)}$, which means $i' = s(i-1) + h$ and $j' = s(i-1) + w$, we find that $i = \\frac {i'-h} {s} + 1$, $j = \\frac {j'-w} {s} + 1$. However, i and j cannot be below 1 as they correspond to the minimum index of the output matrix $dZ^{[l]}$, and it also cannot go beyond its size. In other words, using Python indexing i and j are both floored at 0 and are capped at $n^{[l]}_H - 1$ and $n^{[l]}_W - 1$ respectively. Therefore, $\\forall h \\in [1, f^{[l]}_h]$, $\\forall w \\in [1, f^{[l]}_w]$:  \n",
    "$$ dA^{[l-1]} _{(k, i', j', m)} = \\frac{\\partial Cost}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}} {\\partial A^{[l-1]} _{(k, i', j', m)}} = \\sum_{c=1} ^ {n^{[l]}_C} \\sum _{i=\\max (1, \\lfloor \\frac {i'-h} {s} + 1 \\rfloor)} ^{\\min (i', n^{[l]}_H)} \\sum_{j= \\max (1, \\lfloor \\frac {j'-w} {s} + 1 \\rfloor)} ^ {\\min (j', n^{[l]}_W)} W^{[l]} _{(c, k, h, w)} \\times dZ ^{[l]} _{(c, i, j, m)} \\tag{17} $$  \n",
    "\n",
    "We finally unpad $dA^{[l-1]} _{(k, i', j', m)}$ by the appropriate padding value to come back to the original input matrix size, so that we can carry calculating gradients through (14), (15) and (16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to loop over gradient (dW, dZ) size to calculate dW and db\n",
    "# obs = False for dW as we will already loop through each observation via the slicing function on A\n",
    "# Whereas obs = True for db as we don't use the slicing function as seen in the equation\n",
    "def gradient_loop(dX, layer, type='dW'):\n",
    "    \n",
    "    if type == 'dW':\n",
    "        channel, _, height, width = dX[str(layer)].shape\n",
    "        for c in range(channel):\n",
    "            for h in range(height):\n",
    "                for w in range(width):\n",
    "                    yield c, h, w\n",
    "\n",
    "    elif type == 'dZ':\n",
    "        channel, height, width, observation = dX[str(layer)].shape\n",
    "        for m in range(observation):\n",
    "            for c in range(channel):\n",
    "                for h in range(height):\n",
    "                    for w in range(width):\n",
    "                        yield c, h, w, m\n",
    "    else:\n",
    "        raise ValueError(\"Type needs to be either 'dW' or 'dZ'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_backward_propagation(A, Z, W, dA, dZ, dW, architecture, filter_size, padding, stride):\n",
    "\n",
    "    # Works similar to normal backprop for fully connected layer\n",
    "    # We need to compute the derivative of the last layer separately, as it is a non-generic formula\n",
    "    # 1e-8 is added for numeric stability\n",
    "\n",
    "    L = len(architecture)-1\n",
    "    s = stride\n",
    "    f = filter_size\n",
    "    m = A['0'].shape[3]\n",
    "\n",
    "    dAL = -Y / (A[str(L)] + 1e-8) + (1-Y) / ((1-A[str(L)]) + 1e-8)\n",
    "    dZ[str(L)] = dAL * backward_activation_function(activations[L], Z[str(L)])\n",
    "    dW[str(L)] = 1/m * np.dot(dZ[str(L)], A[str(L-1)].T)\n",
    "    db[str(L)] = 1/m * np.sum(dZ[str(L)], axis=1, keepdims=True)\n",
    "\n",
    "    # We can now compute the generalized backward propagation\n",
    "    for l in range(L-1, 0, -1):\n",
    "    \n",
    "    # ___________________________________ Calculate dA ___________________________________________\n",
    "        # Note that dA is computed differently in function of the previous layer (l+1), but is indifferent of the current layer.\n",
    "        # It is therefore important to compute dA before hand so that we can pass on to the next gradient in the chain.\n",
    "        if architecture[l+1] == 'fc':\n",
    "            dA[str(l)] = np.dot(W[str(l+1)].T, dZ[str(l+1)])\n",
    "\n",
    "        elif architecture[l+1] == 'maxpool':\n",
    "\n",
    "            for slice, i, j, k, o in slicing(dA, A, l, architecture, filter_size, padding, stride, 'backward'):\n",
    "                v1 = i * s[l]\n",
    "                v2 = i * s[l] + f[l]\n",
    "                h1 = j * s[l]\n",
    "                h2 = j * s[l] + f[l]\n",
    "\n",
    "                # The way it works: we take the previous layer gradient dA[l+1].\n",
    "                # Each ith row, jth column, which represents the number which we will allocate to the following gradient dA[l] slice.\n",
    "                # The tip is that the h, w of the hth row and wth column of the following gradient dA[l] is of the same coordinate of the maximum number\n",
    "                # of A[l] e.g. the current output.\n",
    "                # We therefore needs to create a mask which is a boolean matrix of dimension fxf where we have 1 if A[l][h, w] is the maximum of the current\n",
    "                # operating slice, 0 otherwise.\n",
    "                # We multiply this mask to the value of dA[l+1][i, j] so that we have a slice containing the dA[l+1][i, j] located where the maximum of A[l] was.\n",
    "                # The slice is then fully allocated to dA[l].\n",
    "                mask = (slice == np.max(slice))\n",
    "                dA[str(l)][k, v1:v2, h1:h2, o] += mask * vectorization(dA[str(l+1)], A, architecture, l+1, 'matrix')[k, i, j, o]\n",
    "\n",
    "        elif architecture[l+1] == 'avgpool':\n",
    "            for slice, i, j, k, o in slicing(dA, A, l, architecture, filter_size, padding, stride, 'backward'):\n",
    "                v1 = i * s[l]\n",
    "                v2 = i * s[l] + f[l]\n",
    "                h1 = j * s[l]\n",
    "                h2 = j * s[l] + f[l]\n",
    "\n",
    "                # The way it works: we take the previous layer gradient dA[l+1].\n",
    "                # Each ith row, jth column, represents the average of the following gradient dA[l] slice.\n",
    "                # Therefore, for each i, j, we allocate the value of dA[l+1] of the ith row and jth column to the corresponding slice on dA[l]\n",
    "                # And we adjust it by 1/(fxf) to make a proportional allocation.\n",
    "                # Note that the slicing function already iterate up to dA[l+1] height and width, so we can directly use the slice, i, j, k and o\n",
    "                # taken from the slicing function.\n",
    "                dA[str(l)][k, v1:v2, h1:h2, o] += vectorization(dA[str(l+1)], A, architecture, l+1, 'matrix')[k, i, j, o] / np.prod(slice.shape)\n",
    "\n",
    "        elif architecture[l+1] == 'conv':\n",
    "            _, n_h, n_w, _ = dA[str(l+1)].shape\n",
    "            \n",
    "            for c, h, w, o in gradient_loop(dA, l, type='dZ'):\n",
    "                for k, i, j in gradient_loop(W, l+1, type='dW'):\n",
    "                    # The value of dZ[l+1] needs to be clamped within its dimension\n",
    "                    # Note that the stride index is l+1, given it is associated with the convolution layer, which is (l+1)\n",
    "                    z_h = max(0, min( int((h-i)/s[l+1] + 1), n_h-1))\n",
    "                    z_w = max(0, min( int((h-j)/s[l+1] + 1), n_w-1))\n",
    "                    print(\"Current Layer:\", l)\n",
    "                    print(\"For dA:\", c, h, w, o, k)\n",
    "                    print(\"For dZ and W\", i, j, z_h, z_w)\n",
    "                    dA[str(l)][c, h, w, o] += W[str(l+1)][k, c, i, j] * dZ[str(l+1)][k, z_h, z_w, o]\n",
    "\n",
    "\n",
    "    # ___________________________________ Calculate dZ, db, dW ___________________________________________\n",
    "        # Note there is no dZ, dW nor db for Pooling layers\n",
    "        # The fully connected layer is build very similar to the vanilla neural network\n",
    "        if architecture[l] == 'fc':\n",
    "\n",
    "            dZ[str(l)] = dA[str(l)] * backward_activation_function(activations[l], Z[str(l)])\n",
    "            # Need vectorization if A[l-1] is a matrix, when changing from layer pool/conv to fc\n",
    "            dW[str(l)] = 1/m * np.dot(dZ[str(l)], vectorization(A[str(l-1)], A, architecture, l, 'vector').T)\n",
    "            db[str(l)] = 1/m * np.sum(dZ[str(l)], axis=1, keepdims=True)    \n",
    "\n",
    "        elif (architecture[l] == 'conv'):\n",
    "            check = 0\n",
    "            # Need devectorization if the previous layer was 'fc'. Note that A is indexed at layer l but will be a vector if the previous layer is 'fc'.\n",
    "            dZ[str(l)] = vectorization(dA[str(l)], A, architecture, l, 'matrix') * backward_activation_function(activations[l], Z[str(l)])\n",
    "\n",
    "            for c, h, w in gradient_loop(dW, l, type='dW'):\n",
    "                for slice, i, j, k, o in slicing(A, dA, l, architecture, filter_size, padding, stride, 'forward'):\n",
    "                    check += 1\n",
    "                    dW[str(l)][c, k, h, w] += np.sum(dZ[str(l)][c, i, j, o] * slice) / m\n",
    "\n",
    "            for c, h, w, o in gradient_loop(dZ, l, type='dZ'):\n",
    "                db[str(l)][c] += dZ[str(l)][c, h, w, o] / m\n",
    "            print(l, check)\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2\nFor dA: 3 4 3 6 10\nFor dZ and W 2 4 3 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 10\nFor dZ and W 3 0 2 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 10\nFor dZ and W 3 1 2 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 10\nFor dZ and W 3 2 2 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 10\nFor dZ and W 3 3 2 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 10\nFor dZ and W 3 4 2 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 10\nFor dZ and W 4 0 1 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 10\nFor dZ and W 4 1 1 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 10\nFor dZ and W 4 2 1 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 10\nFor dZ and W 4 3 1 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 10\nFor dZ and W 4 4 1 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 0 0 5 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 0 1 5 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 0 2 5 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 0 3 5 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 0 4 5 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 1 0 4 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 1 1 4 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 1 2 4 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 1 3 4 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 1 4 4 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 2 0 3 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 2 1 3 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 2 2 3 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 2 3 3 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 2 4 3 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 3 0 2 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 3 1 2 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 3 2 2 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 3 3 2 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 3 4 2 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 4 0 1 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 4 1 1 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 4 2 1 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 4 3 1 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 11\nFor dZ and W 4 4 1 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 0 0 5 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 0 1 5 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 0 2 5 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 0 3 5 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 0 4 5 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 1 0 4 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 1 1 4 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 1 2 4 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 1 3 4 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 1 4 4 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 2 0 3 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 2 1 3 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 2 2 3 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 2 3 3 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 2 4 3 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 3 0 2 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 3 1 2 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 3 2 2 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 3 3 2 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 3 4 2 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 4 0 1 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 4 1 1 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 4 2 1 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 4 3 1 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 12\nFor dZ and W 4 4 1 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 0 0 5 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 0 1 5 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 0 2 5 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 0 3 5 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 0 4 5 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 1 0 4 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 1 1 4 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 1 2 4 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 1 3 4 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 1 4 4 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 2 0 3 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 2 1 3 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 2 2 3 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 2 3 3 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 2 4 3 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 3 0 2 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 3 1 2 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 3 2 2 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 3 3 2 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 3 4 2 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 4 0 1 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 4 1 1 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 4 2 1 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 4 3 1 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 13\nFor dZ and W 4 4 1 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 0 0 5 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 0 1 5 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 0 2 5 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 0 3 5 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 0 4 5 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 1 0 4 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 1 1 4 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 1 2 4 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 1 3 4 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 1 4 4 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 2 0 3 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 2 1 3 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 2 2 3 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 2 3 3 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 2 4 3 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 3 0 2 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 3 1 2 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 3 2 2 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 3 3 2 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 3 4 2 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 4 0 1 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 4 1 1 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 4 2 1 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 4 3 1 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 14\nFor dZ and W 4 4 1 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 0 0 5 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 0 1 5 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 0 2 5 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 0 3 5 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 0 4 5 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 1 0 4 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 1 1 4 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 1 2 4 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 1 3 4 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 1 4 4 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 2 0 3 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 2 1 3 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 2 2 3 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 2 3 3 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 2 4 3 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 3 0 2 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 3 1 2 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 3 2 2 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 3 3 2 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 3 4 2 1\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 4 0 1 5\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 4 1 1 4\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 4 2 1 3\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 4 3 1 2\nCurrent Layer: 2\nFor dA: 3 4 3 6 15\nFor dZ and W 4 4 1 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 0 0 5 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 0 1 5 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 0 2 5 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 0 3 5 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 0 4 5 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 1 0 4 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 1 1 4 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 1 2 4 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 1 3 4 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 1 4 4 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 2 0 3 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 2 1 3 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 2 2 3 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 2 3 3 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 2 4 3 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 3 0 2 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 3 1 2 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 3 2 2 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 3 3 2 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 3 4 2 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 4 0 1 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 4 1 1 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 4 2 1 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 4 3 1 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 0\nFor dZ and W 4 4 1 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 0 0 5 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 0 1 5 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 0 2 5 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 0 3 5 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 0 4 5 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 1 0 4 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 1 1 4 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 1 2 4 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 1 3 4 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 1 4 4 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 2 0 3 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 2 1 3 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 2 2 3 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 2 3 3 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 2 4 3 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 3 0 2 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 3 1 2 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 3 2 2 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 3 3 2 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 3 4 2 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 4 0 1 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 4 1 1 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 4 2 1 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 4 3 1 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 1\nFor dZ and W 4 4 1 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 0 0 5 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 0 1 5 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 0 2 5 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 0 3 5 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 0 4 5 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 1 0 4 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 1 1 4 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 1 2 4 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 1 3 4 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 1 4 4 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 2 0 3 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 2 1 3 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 2 2 3 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 2 3 3 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 2 4 3 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 3 0 2 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 3 1 2 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 3 2 2 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 3 3 2 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 3 4 2 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 4 0 1 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 4 1 1 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 4 2 1 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 4 3 1 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 2\nFor dZ and W 4 4 1 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 0 0 5 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 0 1 5 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 0 2 5 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 0 3 5 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 0 4 5 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 1 0 4 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 1 1 4 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 1 2 4 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 1 3 4 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 1 4 4 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 2 0 3 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 2 1 3 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 2 2 3 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 2 3 3 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 2 4 3 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 3 0 2 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 3 1 2 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 3 2 2 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 3 3 2 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 3 4 2 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 4 0 1 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 4 1 1 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 4 2 1 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 4 3 1 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 3\nFor dZ and W 4 4 1 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 0 0 5 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 0 1 5 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 0 2 5 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 0 3 5 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 0 4 5 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 1 0 4 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 1 1 4 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 1 2 4 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 1 3 4 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 1 4 4 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 2 0 3 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 2 1 3 4\nCurrent Layer: 2\nFor dA: 34 4 6 4\nFor dZ and W 2 2 3 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 2 3 3 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 2 4 3 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 3 0 2 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 3 1 2 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 3 2 2 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 3 3 2 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 3 4 2 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 4 0 1 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 4 1 1 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 4 2 1 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 4 3 1 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 4\nFor dZ and W 4 4 1 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 0 0 5 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 0 1 5 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 0 2 5 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 0 3 5 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 0 4 5 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 1 0 4 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 1 1 4 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 1 2 4 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 1 3 4 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 1 4 4 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 2 0 3 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 2 1 3 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 2 2 3 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 2 3 3 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 2 4 3 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 3 0 2 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 3 1 2 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 3 2 2 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 3 3 2 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 3 4 2 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 4 0 1 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 4 1 1 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 4 2 1 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 4 3 1 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 5\nFor dZ and W 4 4 1 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 0 0 5 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 0 1 5 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 0 2 5 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 0 3 5 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 0 4 5 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 1 0 4 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 1 1 4 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 1 2 4 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 1 3 4 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 1 4 4 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 2 0 3 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 2 1 3 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 2 2 3 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 2 3 3 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 2 4 3 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 3 0 2 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 3 1 2 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 3 2 2 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 3 3 2 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 3 4 2 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 4 0 1 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 4 1 1 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 4 2 1 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 4 3 1 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 6\nFor dZ and W 4 4 1 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 0 0 5 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 0 1 5 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 0 2 5 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 0 3 5 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 0 4 5 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 1 0 4 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 1 1 4 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 1 2 4 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 1 3 4 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 1 4 4 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 2 0 3 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 2 1 3 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 2 2 3 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 2 3 3 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 2 4 3 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 3 0 2 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 3 1 2 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 3 2 2 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 3 3 2 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 3 4 2 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 4 0 1 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 4 1 1 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 4 2 1 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 4 3 1 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 7\nFor dZ and W 4 4 1 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 0 0 5 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 0 1 5 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 0 2 5 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 0 3 5 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 0 4 5 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 1 0 4 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 1 1 4 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 1 2 4 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 1 3 4 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 1 4 4 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 2 0 3 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 2 1 3 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 2 2 3 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 2 3 3 2\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 2 4 3 1\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 3 0 2 5\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 3 1 2 4\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 3 2 2 3\nCurrent Layer: 2\nFor dA: 3 4 4 6 8\nFor dZ and W 3 3 2 2\n"
    }
   ],
   "source": [
    "dW, db = convolutional_backward_propagation(A, Z, W, dA, dZ, dW, architecture, filter_size, padding, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = update_parameters(len(architecture), W, b, dW, db, learning_rate=0.01)\n",
    "\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using W, b of the previous trained model\n",
    "def predict(X, W, b, nb_layer, activations):\n",
    "\n",
    "    A = {}\n",
    "    A[\"0\"] = X\n",
    "\n",
    "    # We only need one forward propagation\n",
    "    # We must include Z so that the forward propagation function doesn't return a tuple\n",
    "    # However, Z is replaced with \"_\" as it will be unused\n",
    "    _, A = convolutional_forward_propagation(A, Z, W, b, architecture, activations, filter_size, nb_kernel, padding, stride)\n",
    "    Yhat = np.where(A[str(nb_layer)] <= 0.5, 0, 1)\n",
    "\n",
    "    return Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Accuracy function\n",
    "def model_accuracy(Yhat, Y):\n",
    "    # Simply compare the predicted result vs real result\n",
    "    # If it is the same, 1, if not 0. We then take the average\n",
    "    error = np.where(Yhat == Y, 1, 0)    \n",
    "    accuracy = np.mean(error)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the prediction function works well\n",
    "result = predict(X, weight, bias, nb_layer, activations)\n",
    "accuracy = model_accuracy(result, Y)\n",
    "\n",
    "# Tuning back the results from one-hot encoding to categorical values\n",
    "# This will allow us to have a better comparison when comparing with the picture\n",
    "Y = np.where(Y == 0, first, second)\n",
    "result = np.where(result == 0, first, second)\n",
    "\n",
    "print(\"Accuracy of the model: {0:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VI. LeNet-5 Full Model\n",
    "___\n",
    "Now that we have implemented all the necessary function, we will put everything together to create a LeNet-5 Convolutional Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_neural_network(X, Y, architecture, filter_size, nb_kernel, padding, stride, epoch=10, optimization=None, learning_rate=0.01, momentum_beta=0.9, rms_beta=0.999, show_cost=False):\n",
    "    \n",
    "    L = len(architecture) - 1\n",
    "    costs = []\n",
    "    A = {}\n",
    "    A['0'] = X\n",
    "\n",
    "    layer_shape = get_layer_shape(architecture, filter_size, nb_kernel, padding, stride, A)\n",
    "    A, Z, W, b, dA, dZ, dW, db = initialize_model(architecture, filter_size, nb_kernel, padding, stride, A)\n",
    "\n",
    "    for _ in range(epoch):\n",
    "        Z, A = convolutional_forward_propagation(A, Z, W, b, architecture, activations, filter_size, nb_kernel, padding, stride)\n",
    "        cost = compute_cost(A[str(L)], Y)\n",
    "        costs.append(cost)\n",
    "        dW, db = convolutional_backward_propagation(A, Z, W, dA, dZ, dW, architecture, filter_size, padding, stride)\n",
    "\n",
    "        if optimization == None:\n",
    "            W, b = update_parameters(L, W, b, dW, db, learning_rate)\n",
    "        elif optimization == \"momentum\":\n",
    "            W, b = momentum(L, W, b, dW, db, Vw, Vb, momentum_beta, learning_rate)\n",
    "        elif optimization == \"rmsprop\":\n",
    "            W, b = rmsprop(L, W, b, dW, db, Ew, Eb, i+1, rms_beta, learning_rate)\n",
    "        elif optimization == \"adam\":\n",
    "            W, b = adam(L, W, b, dW, db, Vw, Vb, Ew, Eb, i+1, momentum_beta, rms_beta, learning_rate)\n",
    "\n",
    "    if show_cost == True:\n",
    "        # Display the Cost/#Iteration plot\n",
    "        plt.plot(costs)\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.xlabel(\"# Iteration\")\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "    return W, b, costs"
   ]
  }
 ]
}